{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import OrderedDict, defaultdict\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# ================ Pre-compiled Regular Expressions ================\n",
    "RE_WHITESPACE = re.compile(r'\\s+')\n",
    "RE_LEADING_CONJ = re.compile(r'^(?:and|but|or|then|also|so)\\s+')\n",
    "RE_TRAILING_PUNCT = re.compile(r'[,;.]+$')\n",
    "RE_MIDDLE_PUNCT = re.compile(r'[,;]')\n",
    "RE_END_CONJ = re.compile(r'\\s+(?:while|as|when|and|but|or|after|before|since|because)$')\n",
    "\n",
    "# ================ Pre-defined Dictionaries ================\n",
    "# Load from pickle file\n",
    "with open('FAVOR_Dictionary/action_extraction_dicts.pkl', 'rb') as f:\n",
    "    dict_data = pickle.load(f)\n",
    "\n",
    "# Extract various dictionaries\n",
    "ANIMATE_NOUNS = dict_data[\"ANIMATE_NOUNS\"]\n",
    "SUBJECT_SYNONYMS = dict_data[\"SUBJECT_SYNONYMS\"]\n",
    "PLURAL_FORMS = dict_data[\"PLURAL_FORMS\"]\n",
    "COMMON_VERBS = dict_data[\"COMMON_VERBS\"]\n",
    "PHRASAL_VERBS = dict_data[\"PHRASAL_VERBS\"]\n",
    "IRREGULAR_VERB_FORMS = dict_data[\"IRREGULAR_VERB_FORMS\"]\n",
    "CAMERA_TERMS = dict_data[\"CAMERA_TERMS\"]\n",
    "DESCRIPTIVE_ADJECTIVES = dict_data[\"DESCRIPTIVE_ADJECTIVES\"]\n",
    "COLOR_TERMS = dict_data[\"COLOR_TERMS\"]\n",
    "CLOTHING_TERMS = dict_data[\"CLOTHING_TERMS\"]\n",
    "SETTING_TERMS = dict_data[\"SETTING_TERMS\"]\n",
    "POSITION_TERMS = dict_data[\"POSITION_TERMS\"]\n",
    "EMOTION_TERMS = dict_data[\"EMOTION_TERMS\"]\n",
    "\n",
    "# Pre-compile sets for efficient lookup\n",
    "COMMON_VERBS_SET = set(COMMON_VERBS)\n",
    "CAMERA_TERMS_SET = set(CAMERA_TERMS)\n",
    "\n",
    "\n",
    "# ================ Helper Functions ================\n",
    "def normalize_verb(verb_form):\n",
    "    \"\"\"Convert a verb form to its base form\"\"\"\n",
    "    # Check irregular verb mappings\n",
    "    for base_form, forms in IRREGULAR_VERB_FORMS.items():\n",
    "        if verb_form.lower() in forms:\n",
    "            return base_form\n",
    "        \n",
    "    # Handle regular verbs\n",
    "    verb = verb_form.lower()\n",
    "    \n",
    "    # Handle -ing forms\n",
    "    if verb.endswith('ing'):\n",
    "        # Double consonant ending + ing\n",
    "        if len(verb) > 4 and verb[-4] == verb[-5] and verb[-4] not in 'aeiou':\n",
    "            return verb[:-4]\n",
    "        # Drop e + ing\n",
    "        elif len(verb) > 5 and verb[:-3] + 'e' in COMMON_VERBS_SET:\n",
    "            return verb[:-3] + 'e'\n",
    "        # Standard -ing\n",
    "        else:\n",
    "            return verb[:-3]\n",
    "    \n",
    "    # Handle -ed forms\n",
    "    if verb.endswith('ed'):\n",
    "        # Double consonant ending + ed\n",
    "        if len(verb) > 3 and verb[-3] == verb[-4] and verb[-3] not in 'aeiou':\n",
    "            return verb[:-3]\n",
    "        # y -> ied\n",
    "        elif verb.endswith('ied') and len(verb) > 3:\n",
    "            return verb[:-3] + 'y'\n",
    "        # Standard -ed\n",
    "        elif verb[:-2] + 'e' in COMMON_VERBS_SET:\n",
    "            return verb[:-2] + 'e'\n",
    "        else:\n",
    "            return verb[:-2]\n",
    "    \n",
    "    # Handle third person singular -s forms\n",
    "    if verb.endswith('s') and not verb.endswith('ss'):\n",
    "        # -ies form\n",
    "        if verb.endswith('ies'):\n",
    "            return verb[:-3] + 'y'\n",
    "        # -es form\n",
    "        elif verb.endswith(('ches', 'shes', 'sses', 'xes', 'zes')):\n",
    "            return verb[:-2]\n",
    "        # Simple -s form\n",
    "        else:\n",
    "            return verb[:-1]\n",
    "    \n",
    "    # If no rules match, return as is\n",
    "    return verb\n",
    "\n",
    "def is_potential_verb(word):\n",
    "    \"\"\"Quick check if a word could be a verb\"\"\"\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Direct check in predefined verb list\n",
    "    if word in COMMON_VERBS_SET:\n",
    "        return True\n",
    "    \n",
    "    # Check for possible verb inflections\n",
    "    if word.endswith(('s', 'ed', 'ing')):\n",
    "        # Try to extract base form\n",
    "        base = normalize_verb(word)\n",
    "        if base in COMMON_VERBS_SET:\n",
    "            return True\n",
    "    \n",
    "    # Check irregular verb forms\n",
    "    for forms in IRREGULAR_VERB_FORMS.values():\n",
    "        if word in forms:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Pre-compile complex regex patterns and templates\n",
    "def compile_all_patterns():\n",
    "    \"\"\"Pre-compile all commonly used regex patterns\"\"\"\n",
    "    patterns = {}\n",
    "    \n",
    "    # 1. Specific camera motion patterns\n",
    "    specific_camera_patterns = [\n",
    "        r'[Tt]he camera then pans',\n",
    "        r'[Tt]he camera shifts',\n",
    "        r'[Tt]he camera follows',\n",
    "        r'[Tt]he camera pans',\n",
    "        r'[Tt]he camera then shifts',\n",
    "        r'[Tt]he scene transitions',\n",
    "        r'[Tt]he camera (?:slowly|quickly|gradually)',\n",
    "        r'[Tt]he camera moves',\n",
    "        r'[Tt]he camera (?:remains|stays)',\n",
    "        r'[Tt]he camera (?:is|appears to be)',\n",
    "        r'[Tt]he camera pans',\n",
    "        r'[Tt]he camera continues',\n",
    "        r'[Tt]he camera zooms',\n",
    "        r'[Tt]he camera (?:tracks|follows)',\n",
    "        r'[Aa] (?:tracking|panning|zoom|dolly) shot',\n",
    "        r'[Tt]he (?:shot|frame|video|scene) transitions',\n",
    "        r'[Tt]he (?:shot|frame|video|scene) changes',\n",
    "        r'[Tt]he (?:focus|angle|perspective) shifts',\n",
    "        r'[Tt]he video starts with',\n",
    "        r'[Tt]he video begins with',\n",
    "        r'[Tt]he video shows',\n",
    "        r'[Tt]he scene begins with',\n",
    "        r'[Tt]he scene starts with',\n",
    "        r'[Cc]amera (?:then )?cuts to',\n",
    "        r'[Tt]he static shot',\n",
    "        r'[Ss]tationary shot',\n",
    "        r'[Ss]tatic (?:shot|camera)',\n",
    "        r'[Cc]apturing the.*from',\n",
    "        r'[Oo]verhead (?:shot|view)',\n",
    "        r'[Ff]irst-person perspective',\n",
    "    ]\n",
    "    \n",
    "    # 2. General camera motion patterns\n",
    "    general_camera_patterns = [\n",
    "        r'(?:the\\s+)?camera\\s+(?:pan|pans|panned|panning)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:zoom|zooms|zoomed|zooming)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:follow|follows|followed|following)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:shift|shifts|shifted|shifting)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:tilt|tilts|tilted|tilting)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:track|tracks|tracked|tracking)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:shake|shakes|shook|shaking)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:remain|remains|remained|keep|keeps|kept)\\s+(?:static|stationary|fixed|steady)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:move|moves|moved|moving)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:is|was)\\s+(?:positioned|placed|located)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:capture|captures|captured|capturing)',\n",
    "        r'(?:the\\s+)?camera\\s+(?:focus|focuses|focused|focusing)',\n",
    "        r'(?:a|the)\\s+(?:static|stationary|moving|panning|tracking|zoom)\\s+shot',\n",
    "        r'(?:a|the)\\s+(?:close-up|medium|wide|aerial|overhead)\\s+shot',\n",
    "        r'[Tt]he scene (?:remains|stays) static',\n",
    "        r'[Tt]he scene (?:is|appears) static',\n",
    "        r'stationary (?:throughout|camera)',\n",
    "        r'no significant camera movement',\n",
    "        r'no camera movement',\n",
    "        r'steady camera',\n",
    "        r'stable camera',\n",
    "        r'fixed camera',\n",
    "    ]\n",
    "    \n",
    "    # Compile camera patterns\n",
    "    patterns['camera_patterns'] = [re.compile(p) for p in specific_camera_patterns + general_camera_patterns]\n",
    "    \n",
    "    # 3. Descriptive statement patterns\n",
    "    descriptive_patterns = [\n",
    "        r'^(?:is|are|was|were)\\s+(?:visible|present|shown|displayed)',\n",
    "        r'^(?:seems|seemed|appears|appeared)\\s+to\\s+be',\n",
    "        r'^(?:looks|looked)\\s+(?:like|similar)',\n",
    "        r'^(?:suggesting|indicating|showing|depicting)',\n",
    "        r'^(?:can\\s+be\\s+seen)',\n",
    "        r'^(?:wearing|carrying|holding)\\b',\n",
    "        r'^(?:stands|standing|seated|sitting|lying)\\s+in\\s',\n",
    "        r'what appears to be',\n",
    "        r'appears to be',\n",
    "        r'seems to be'\n",
    "    ]\n",
    "    patterns['descriptive_patterns'] = [re.compile(p, re.IGNORECASE) for p in descriptive_patterns]\n",
    "    \n",
    "    # 4. Scene description patterns\n",
    "    scene_patterns = [\n",
    "        r'^the\\s+(?:scene|background|setting|video|clip)\\s+',\n",
    "        r'^the\\s+video\\s+(?:shows|depicts|begins|starts|opens)\\s+',\n",
    "        r'^in\\s+the\\s+(?:background|foreground|scene|setting)\\s+',\n",
    "        r'^the\\s+(?:room|area|space|environment)\\s+',\n",
    "        r'\\b(?:appears|appears\\s+to\\s+be|looks\\s+like)\\b',\n",
    "        r'\\b(?:suggesting|indicating|depicting)\\b'\n",
    "    ]\n",
    "    patterns['scene_patterns'] = [re.compile(p, re.IGNORECASE) for p in scene_patterns]\n",
    "    \n",
    "    # 5. Subject indicator patterns\n",
    "    subject_indicators = [\n",
    "        r'\\bthe\\s+(?:\\w+\\s+)*(?:man|woman|person|boy|girl|child|player|individual)',\n",
    "        r'\\ba\\s+(?:\\w+\\s+)*(?:man|woman|person|boy|girl|child|player|individual)',\n",
    "        r'\\ban\\s+(?:\\w+\\s+)*(?:man|woman|person|boy|girl|child|player|individual)'\n",
    "    ]\n",
    "    patterns['subject_indicators'] = [re.compile(p, re.IGNORECASE) for p in subject_indicators]\n",
    "    \n",
    "    # 6. Passive voice patterns\n",
    "    passive_constructions = [\n",
    "        r'is\\s+\\w+ed', r'was\\s+\\w+ed', r'are\\s+\\w+ed', r'were\\s+\\w+ed',\n",
    "        r'is\\s+being\\s+\\w+ed', r'was\\s+being\\s+\\w+ed', \n",
    "        r'has\\s+been\\s+\\w+ed', r'have\\s+been\\s+\\w+ed',\n",
    "        r'had\\s+been\\s+\\w+ed',\n",
    "        r'is\\s+\\w+en', r'was\\s+\\w+en', r'are\\s+\\w+en', r'were\\s+\\w+en'\n",
    "    ]\n",
    "    patterns['passive_patterns'] = [re.compile(p, re.IGNORECASE) for p in passive_constructions]\n",
    "    \n",
    "    # 7. State and emotion change patterns\n",
    "    state_patterns = [\n",
    "        (r'expression\\s+changes?\\s+from\\s+([^.,;:]+)\\s+to\\s+([^.,;:]+)', \n",
    "         lambda m: f\"expression changes from {m.group(1)} to {m.group(2)}\"),\n",
    "        (r'expression\\s+changes?\\s+to\\s+([^.,;:]+)', \n",
    "         lambda m: f\"expression changes to {m.group(1)}\"),\n",
    "        (r'(?:indicating|suggesting)\\s+(?:he|she|they)\\s+is\\s+([^.,;:]+)', \n",
    "         lambda m: f\"is {m.group(1)}\"),\n",
    "        (r'appears\\s+to\\s+be\\s+([^.,;:]+)', \n",
    "         lambda m: f\"appears to be {m.group(1)}\"),\n",
    "        (r'seems\\s+to\\s+be\\s+([^.,;:]+)', \n",
    "         lambda m: f\"seems to be {m.group(1)}\"),\n",
    "        (r'continues\\s+to\\s+([^.,;:]+)', \n",
    "         lambda m: f\"continues to {m.group(1)}\"),\n",
    "        (r'remains\\s+([^.,;:]+)', \n",
    "         lambda m: f\"remains {m.group(1)}\")\n",
    "    ]\n",
    "    patterns['state_patterns'] = [(re.compile(p, re.IGNORECASE), f) for p, f in state_patterns]\n",
    "    \n",
    "    # 8. Reaction and interaction patterns\n",
    "    reaction_patterns = [\n",
    "        (r'reacting\\s+to\\s+([^.,;:]+)', \n",
    "         lambda m: f\"reacting to {m.group(1)}\"),\n",
    "        (r'responding\\s+to\\s+([^.,;:]+)', \n",
    "         lambda m: f\"responding to {m.group(1)}\"),\n",
    "        (r'engaged\\s+with\\s+([^.,;:]+)', \n",
    "         lambda m: f\"engaged with {m.group(1)}\"),\n",
    "        (r'looks?\\s+(?:at|towards|to)\\s+([^.,;:]+)\\s+with\\s+interest', \n",
    "         lambda m: f\"looks at {m.group(1)} with interest\")\n",
    "    ]\n",
    "    patterns['reaction_patterns'] = [(re.compile(p, re.IGNORECASE), f) for p, f in reaction_patterns]\n",
    "    \n",
    "    # 9. Pre-compile verb patterns\n",
    "    verb_pattern_parts = set()\n",
    "    \n",
    "    # Add all common verbs and their inflections\n",
    "    for verb in COMMON_VERBS:\n",
    "        # Base form\n",
    "        verb_pattern_parts.add(verb)\n",
    "        \n",
    "        # 3rd person singular\n",
    "        if verb.endswith(('s', 'x', 'z', 'ch', 'sh')):\n",
    "            verb_pattern_parts.add(f\"{verb}es\")\n",
    "        elif verb.endswith('y') and verb[-2] not in 'aeiou':\n",
    "            verb_pattern_parts.add(f\"{verb[:-1]}ies\")\n",
    "        else:\n",
    "            verb_pattern_parts.add(f\"{verb}s\")\n",
    "        \n",
    "        # Past tense (regular verbs)\n",
    "        if verb.endswith('e'):\n",
    "            verb_pattern_parts.add(f\"{verb}d\")\n",
    "        elif verb.endswith('y') and verb[-2] not in 'aeiou':\n",
    "            verb_pattern_parts.add(f\"{verb[:-1]}ied\")\n",
    "        elif len(verb) > 2 and verb[-1] not in 'aeiou' and verb[-2] in 'aeiou' and verb[-3] not in 'aeiou':\n",
    "            verb_pattern_parts.add(f\"{verb}{verb[-1]}ed\")  # Double consonant\n",
    "        else:\n",
    "            verb_pattern_parts.add(f\"{verb}ed\")\n",
    "        \n",
    "        # Present participle\n",
    "        if verb.endswith('e') and not verb.endswith('ee'):\n",
    "            verb_pattern_parts.add(f\"{verb[:-1]}ing\")\n",
    "        elif len(verb) > 2 and verb[-1] not in 'aeiou' and verb[-2] in 'aeiou' and verb[-3] not in 'aeiou':\n",
    "            verb_pattern_parts.add(f\"{verb}{verb[-1]}ing\")  # Double consonant\n",
    "        else:\n",
    "            verb_pattern_parts.add(f\"{verb}ing\")\n",
    "    \n",
    "    # Add irregular verb forms\n",
    "    for base_form, forms in IRREGULAR_VERB_FORMS.items():\n",
    "        for form in forms:\n",
    "            verb_pattern_parts.add(form)\n",
    "    \n",
    "    # Add phrasal verbs\n",
    "    for phrasal_verb in PHRASAL_VERBS:\n",
    "        verb_pattern_parts.add(phrasal_verb)\n",
    "    \n",
    "    # Convert to regex patterns and compile\n",
    "    patterns['verb_patterns'] = [re.compile(r'\\b' + re.escape(word) + r'\\b', re.IGNORECASE) for word in verb_pattern_parts]\n",
    "    \n",
    "    # 10. Specific action patterns\n",
    "    specific_actions = [\n",
    "        (r'searching\\s+through\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'searching\\s+for\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'looking\\s+through\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'looking\\s+for\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'walks\\s+to\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'walking\\s+to\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'runs\\s+to\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'running\\s+to\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'moves\\s+to\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'moving\\s+to\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'looks\\s+(?:at|to|towards)\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'looking\\s+(?:at|to|towards)\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'bends\\s+down\\s+[^.,;:]*', lambda m: m.group(0)),\n",
    "        (r'picks\\s+up\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'puts\\s+(?:down|away|on|in)\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'reacting\\s+to\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'speaking\\s+(?:to|with)\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'ensures\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'adjusts\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'handles\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'examines\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'interacts\\s+with\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'gestures\\s+(?:to|towards|at)\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'engaged\\s+in\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'reaches\\s+(?:for|out to|towards)\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'hands\\s+over\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'passes\\s+[^.,;:]+', lambda m: m.group(0)),\n",
    "        (r'continues\\s+to\\s+[^.,;:]+', lambda m: m.group(0))\n",
    "    ]\n",
    "    patterns['specific_actions'] = [(re.compile(p, re.IGNORECASE), f) for p, f in specific_actions]\n",
    "    \n",
    "    # 11. Additional verb patterns\n",
    "    action_indicators = [\n",
    "        ('ensures', r'\\bensures\\s+([^.,;:]+)'),\n",
    "        ('moves', r'\\bmoves\\s+([^.,;:]+)'),\n",
    "        ('adjusts', r'\\badjusts\\s+([^.,;:]+)'),\n",
    "        ('observes', r'\\bobserves\\s+([^.,;:]+)'),\n",
    "        ('watches', r'\\bwatches\\s+([^.,;:]+)'),\n",
    "        ('continues', r'\\bcontinues\\s+to\\s+([^.,;:]+)'),\n",
    "        ('begins', r'\\bbegins\\s+to\\s+([^.,;:]+)'),\n",
    "        ('starts', r'\\bstarts\\s+to\\s+([^.,;:]+)'),\n",
    "        ('tries', r'\\btries\\s+to\\s+([^.,;:]+)'),\n",
    "        ('attempts', r'\\battempts\\s+to\\s+([^.,;:]+)')\n",
    "    ]\n",
    "    patterns['action_indicators'] = [(kw, re.compile(p, re.IGNORECASE)) for kw, p in action_indicators]\n",
    "    \n",
    "    # 12. Create a set of -ing noun compounds\n",
    "    ing_compounds = {\n",
    "        'cutting board', 'serving plate', 'serving dish', 'serving tray', 'serving spoon',\n",
    "        'cooking pot', 'cooking pan', 'cooking utensil', 'cooking oil', 'cooking spray',\n",
    "        'baking dish', 'baking sheet', 'baking pan', 'baking powder',\n",
    "        'rolling pin', 'shopping cart', 'shopping bag', 'shopping basket',\n",
    "        'running water', 'drinking water', 'cooking water', 'cooking wine',\n",
    "        'bottled water', 'sparkling water', 'cleaning solution', 'cleaning agent',\n",
    "        'packing material', 'wrapping paper', 'writing paper',\n",
    "        'dining room', 'dining table', 'dining area', 'dining chair',\n",
    "        'living room', 'living area', 'living space',\n",
    "        'waiting room', 'meeting room', 'dressing room',\n",
    "        'cooking show', 'cooking class', 'cooking lesson', 'cooking skill',\n",
    "        'cooking technique', 'cooking time', 'cooking temperature',\n",
    "        'serving size', 'serving style', 'serving method',\n",
    "        'serving motion', 'serving technique', 'serving position',\n",
    "        'training session', 'opening ceremony', 'closing ceremony',\n",
    "        'sporting event', 'running track', 'swimming pool',\n",
    "        'running shoes', 'swimming suit', 'diving board',\n",
    "        'starting position', 'ending position', 'turning point',\n",
    "        'breaking point', 'meeting point', 'resting place',\n",
    "        'turning mechanism', 'operating system', 'starting line',\n",
    "        'finishing line', 'measuring device', 'counting machine',\n",
    "        'serving line', 'playing field', 'scoring position',\n",
    "        'bowling alley', 'racing track', 'jumping obstacle',\n",
    "        'putting green', 'shooting range', 'batting cage',\n",
    "        'serving position', 'playing court', 'training ground'\n",
    "    }\n",
    "    patterns['ing_compounds'] = ing_compounds\n",
    "    \n",
    "    # Convert to set for faster lookup\n",
    "    patterns['ing_compounds_set'] = set(ing_compounds)\n",
    "    \n",
    "    # 13. Attribute patterns\n",
    "    attribute_patterns = [\n",
    "        # Clothing\n",
    "        (r'wearing\\s+([^.,;:?!]+)', lambda m: \"wearing \" + m.group(1).strip()),\n",
    "        (r'dressed\\s+in\\s+([^.,;:?!]+)', lambda m: \"dressed in \" + m.group(1).strip()),\n",
    "        \n",
    "        # Held items\n",
    "        (r'holding\\s+([^.,;:?!]+)', lambda m: \"holding \" + m.group(1).strip()),\n",
    "        (r'carrying\\s+([^.,;:?!]+)', lambda m: \"carrying \" + m.group(1).strip()),\n",
    "        \n",
    "        # Features with \"with\"\n",
    "        (r'with\\s+([^.,;:?!]+)', lambda m: \"with \" + m.group(1).strip()),\n",
    "        \n",
    "        # Location or place\n",
    "        (r'in\\s+([^.,;:?!]+(?:shirt|dress|jacket|uniform|clothing|clothes|outfit))', lambda m: \"in \" + m.group(1).strip()),\n",
    "        (r'in\\s+([^.,;:?!]+)', lambda m: \"in \" + m.group(1).strip()),  # More general \"in\" pattern\n",
    "        \n",
    "        # Appositive attributes in commas\n",
    "        (r',\\s+([^.,;:?!]*(?:wearing|dressed|holding|carrying|with)[^.,;:?!]*)', lambda m: m.group(1).strip())\n",
    "    ]\n",
    "    patterns['attribute_patterns'] = [(re.compile(p, re.IGNORECASE), f) for p, f in attribute_patterns]\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "def nltk_ensure_downloaded():\n",
    "    \"\"\"Ensure required NLTK resources are downloaded\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "\n",
    "    try:\n",
    "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    except LookupError:\n",
    "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Get all possible subject nouns and their plural forms\n",
    "def get_subject_nouns():\n",
    "    \"\"\"Return all possible subject nouns and their plural forms\"\"\"\n",
    "    all_nouns = set(ANIMATE_NOUNS)\n",
    "    \n",
    "    # Add regular and irregular plural forms\n",
    "    for noun in list(all_nouns):  # Use list copy to avoid modifying set during iteration\n",
    "        # Add irregular plural forms\n",
    "        if noun in PLURAL_FORMS:\n",
    "            all_nouns.add(PLURAL_FORMS[noun])\n",
    "        # Add regular plural forms\n",
    "        elif not noun.endswith('s'):\n",
    "            if noun.endswith(('s', 'x', 'z', 'ch', 'sh')):\n",
    "                all_nouns.add(f\"{noun}es\")\n",
    "            else:\n",
    "                all_nouns.add(f\"{noun}s\")\n",
    "    \n",
    "    # Also add plural forms for synonyms\n",
    "    for noun, synonyms in SUBJECT_SYNONYMS.items():\n",
    "        for synonym in synonyms:\n",
    "            all_nouns.add(synonym)\n",
    "            # Add irregular plural forms\n",
    "            if synonym in PLURAL_FORMS:\n",
    "                all_nouns.add(PLURAL_FORMS[synonym])\n",
    "            # Add regular plural forms\n",
    "            elif not synonym.endswith('s'):\n",
    "                if synonym.endswith(('s', 'x', 'z', 'ch', 'sh')):\n",
    "                    all_nouns.add(f\"{synonym}es\")\n",
    "                else:\n",
    "                    all_nouns.add(f\"{synonym}s\")\n",
    "    \n",
    "    return sorted(all_nouns, key=len, reverse=True)  # Sort by length, longer ones first\n",
    "\n",
    "# ================ Initialize Common Resources ================\n",
    "# Pre-compiled regular expressions and patterns\n",
    "COMPILED_PATTERNS = compile_all_patterns()\n",
    "SUBJECT_NOUNS = get_subject_nouns()\n",
    "SUBJECT_NOUNS_SET = set(SUBJECT_NOUNS)  # For fast lookups\n",
    "\n",
    "class VideoDescriptionParser:\n",
    "    \"\"\"Class for parsing video descriptions to extract structured information\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Make sure necessary NLTK resources are downloaded\n",
    "        nltk_ensure_downloaded()\n",
    "        \n",
    "        # Initialize caches\n",
    "        self.reset_cache()\n",
    "        \n",
    "        # Store globally compiled patterns\n",
    "        self.patterns = COMPILED_PATTERNS\n",
    "        \n",
    "    def reset_cache(self):\n",
    "        \"\"\"Reset all caches to prepare for parsing a new description\"\"\"\n",
    "        # Cache for verb checks\n",
    "        self.verb_check_cache = {}\n",
    "        \n",
    "        # Caches for subject identification\n",
    "        self.cached_subjects = None\n",
    "        self.cached_subject_refs = None\n",
    "        \n",
    "        # Camera motion caches\n",
    "        self.cached_camera_motions = []\n",
    "        self.cached_camera_texts = []\n",
    "        \n",
    "        # Action extraction cache\n",
    "        self.action_extraction_cache = {}\n",
    "        \n",
    "        # Caches for subject and pronoun lookups\n",
    "        self.subject_lookup_cache = {}\n",
    "        self.pronoun_cache = {}\n",
    "\n",
    "    def contains_verb(self, text):\n",
    "        \"\"\"Check if text contains any verb, using cache for performance\"\"\"\n",
    "        # Check cache\n",
    "        if text in self.verb_check_cache:\n",
    "            return self.verb_check_cache[text]\n",
    "        \n",
    "        # Quick check for common verbs\n",
    "        words = text.lower().split()\n",
    "        if any(word in COMMON_VERBS_SET for word in words):\n",
    "            self.verb_check_cache[text] = True\n",
    "            return True\n",
    "        \n",
    "        # Use regex for detailed checking\n",
    "        has_verb = any(pattern.search(text) for pattern in self.patterns['verb_patterns'])\n",
    "        self.verb_check_cache[text] = has_verb\n",
    "        return has_verb\n",
    "    \n",
    "    def extract_main_verb(self, text):\n",
    "        \"\"\"Extract the main verb from an action text\"\"\"\n",
    "        # First check for phrasal verbs\n",
    "        for phrasal_verb in PHRASAL_VERBS:\n",
    "            if re.search(r'\\b' + re.escape(phrasal_verb) + r'\\b', text, re.IGNORECASE):\n",
    "                return phrasal_verb\n",
    "        \n",
    "        # Then check all verb patterns\n",
    "        for pattern in self.patterns['verb_patterns']:\n",
    "            match = pattern.search(text)\n",
    "            if match:\n",
    "                verb = match.group(0)\n",
    "                \n",
    "                # Check for phrasal verbs (like \"pick up\")\n",
    "                post_verb = text[match.end():].lstrip()\n",
    "                prepositions = [\"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \n",
    "                               \"through\", \"away\", \"back\", \"forward\", \"backward\", \"to\", \"at\"]\n",
    "                \n",
    "                for prep in prepositions:\n",
    "                    if re.match(r'^\\s*' + prep + r'\\b', post_verb):\n",
    "                        if verb.lower() not in ['is', 'are', 'was', 'were', 'be', 'been', 'being']:\n",
    "                            verb += f\" {prep}\"\n",
    "                            break\n",
    "                        \n",
    "                return verb\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def is_descriptive_statement(self, text):\n",
    "        \"\"\"Check if the text is descriptive rather than action-oriented\"\"\"\n",
    "        # Use precompiled patterns to check\n",
    "        for pattern in self.patterns['descriptive_patterns']:\n",
    "            if pattern.search(text):\n",
    "                return True\n",
    "        \n",
    "        # Check for be-verb followed by adjective\n",
    "        be_verb_adj = re.search(r'^(?:is|are|was|were)\\s+(\\w+)', text.lower())\n",
    "        if be_verb_adj:\n",
    "            adj = be_verb_adj.group(1)\n",
    "            if adj in DESCRIPTIVE_ADJECTIVES:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def is_ing_nominal_compound(self, text):\n",
    "        \"\"\"Check if text might be an -ing form noun compound rather than action\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Quick check against predefined compounds set\n",
    "        for compound in self.patterns['ing_compounds_set']:\n",
    "            if compound in text_lower:\n",
    "                return True\n",
    "        \n",
    "        # Check patterns that might indicate noun compounds\n",
    "        nominal_patterns = [\n",
    "            # -ing words modifying nouns\n",
    "            r'\\b(?:the|a|an|this|that|these|those|my|your|his|her|their|our)\\s+(\\w+ing)\\s+(\\w+)\\b',\n",
    "            \n",
    "            # Preposition followed by -ing form (likely gerund)\n",
    "            r'\\b(?:on|in|at|by|with|from|to|through|about|after|before)\\s+(?:the\\s+)?(\\w+ing)\\b',\n",
    "            \n",
    "            # Possessive followed by -ing form (clearly gerund)\n",
    "            r\"\\b(?:his|her|their|its|one's)\\s+(\\w+ing)\\b\",\n",
    "            \n",
    "            # -ing form followed by common nouns that often form compounds\n",
    "            r'\\b(\\w+ing)\\s+(?:room|table|board|surface|line|position|style|technique|machine|device|system|method|process|approach)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in nominal_patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                # Make sure this isn't part of a progressive verb structure\n",
    "                if not re.search(r'\\bis\\s+\\w+ing\\b', text_lower) and not re.search(r'\\bwas\\s+\\w+ing\\b', text_lower):\n",
    "                    return True\n",
    "        \n",
    "        # Process-related ing forms when used as nouns\n",
    "        process_ing_forms = [\n",
    "            'cooking', 'baking', 'frying', 'boiling', 'washing', 'cleaning',\n",
    "            'swimming', 'running', 'jumping', 'serving', 'riding', 'playing',\n",
    "            'training', 'learning', 'teaching', 'writing', 'reading', 'speaking',\n",
    "            'working', 'traveling', 'studying', 'thinking', 'computing', 'measuring'\n",
    "        ]\n",
    "        \n",
    "        for ing_form in process_ing_forms:\n",
    "            # If ing form appears as noun (with determiner)\n",
    "            if re.search(rf'\\b(?:the|a|an|this|that|some|any|all)\\s+{ing_form}\\b', text_lower):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def clean_action_text(self, action):\n",
    "        \"\"\"Clean action text by removing punctuation and handling passive voice correctly\"\"\"\n",
    "        # Remove any trailing commas, periods, semicolons\n",
    "        action = RE_TRAILING_PUNCT.sub('', action).strip()\n",
    "        \n",
    "        # Remove any middle commas, semicolons\n",
    "        action = RE_MIDDLE_PUNCT.sub(' ', action).strip()\n",
    "        \n",
    "        # Remove excess whitespace\n",
    "        action = RE_WHITESPACE.sub(' ', action).strip()\n",
    "        \n",
    "        # Handle passive voice - identify passive structures and keep them intact\n",
    "        is_passive = False\n",
    "        for pattern in self.patterns['passive_patterns']:\n",
    "            if pattern.search(action):\n",
    "                is_passive = True\n",
    "                break\n",
    "        \n",
    "        if not is_passive:\n",
    "            # For active voice, truncate at connectors if present\n",
    "            connectors = ['while', 'as', 'when', 'and', 'but', 'or', 'after', 'before', 'since', 'because']\n",
    "            for connector in connectors:\n",
    "                pattern = r'\\s+' + connector + r'\\s+'\n",
    "                if re.search(pattern, action):\n",
    "                    parts = re.split(pattern, action, 1)\n",
    "                    action = parts[0].strip()\n",
    "                    break\n",
    "        \n",
    "        # Make sure there's no trailing conjunction\n",
    "        action = RE_END_CONJ.sub('', action).strip()\n",
    "        \n",
    "        # Remove leading conjunctions\n",
    "        action = RE_LEADING_CONJ.sub('', action).strip()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def extract_camera_motions_from_sentence(self, sentence):\n",
    "        \"\"\"Extract complete camera motion descriptions from a sentence, avoiding duplicates\"\"\"\n",
    "        # Quick check if this sentence might contain camera motion\n",
    "        if not any(term in sentence.lower() for term in CAMERA_TERMS_SET):\n",
    "            return [], []\n",
    "        \n",
    "        camera_motions = []\n",
    "        text_to_remove = []\n",
    "        processed_spans = set()  # To track text ranges already processed\n",
    "        \n",
    "        # Use precompiled camera patterns\n",
    "        for pattern in self.patterns['camera_patterns']:\n",
    "            matches = list(pattern.finditer(sentence))\n",
    "            for match in matches:\n",
    "                motion_start = match.start()\n",
    "                \n",
    "                # Check if this position is already processed\n",
    "                already_processed = False\n",
    "                for start, end in processed_spans:\n",
    "                    if motion_start >= start and motion_start < end:\n",
    "                        already_processed = True\n",
    "                        break\n",
    "                \n",
    "                if already_processed:\n",
    "                    continue\n",
    "                \n",
    "                motion_end = self.find_camera_motion_end(sentence, match.end())\n",
    "                span = (motion_start, motion_end)\n",
    "                processed_spans.add(span)\n",
    "                \n",
    "                full_motion = sentence[motion_start:motion_end].strip()\n",
    "                normalized = self.normalize_camera_motion(full_motion)\n",
    "                if normalized and normalized not in camera_motions:\n",
    "                    camera_motions.append(normalized)\n",
    "                    text_to_remove.append(full_motion)\n",
    "        \n",
    "        return camera_motions, text_to_remove\n",
    "    \n",
    "    def find_camera_motion_end(self, sentence, start_idx):\n",
    "        \"\"\"Find the end position of camera motion description and where subject action begins\"\"\"\n",
    "        # Look for specific markers indicating subject description or action\n",
    "        subject_markers = [\n",
    "            r'\\ba\\s+(?:man|woman|person|boy|girl)', \n",
    "            r'\\bthe\\s+(?:man|woman|person|boy|girl)',\n",
    "            r'\\bhe\\b', r'\\bshe\\b', r'\\bthey\\b',\n",
    "            r'\\bshowing\\b', r'\\bdisplaying\\b', r'\\brevealing\\b',\n",
    "            r'\\bas\\b', r'\\bwhile\\b'\n",
    "        ]\n",
    "        \n",
    "        earliest_marker = len(sentence)\n",
    "        \n",
    "        # Find earliest subject marker\n",
    "        for marker in subject_markers:\n",
    "            match = re.search(marker, sentence[start_idx:], re.IGNORECASE)\n",
    "            if match and start_idx + match.start() < earliest_marker:\n",
    "                earliest_marker = start_idx + match.start()\n",
    "        \n",
    "        # Also check for punctuation\n",
    "        for i in range(start_idx, min(len(sentence), earliest_marker)):\n",
    "            if sentence[i] in '.,:;':\n",
    "                return i\n",
    "        \n",
    "        # Return earliest marker position or end of sentence\n",
    "        return earliest_marker if earliest_marker < len(sentence) else len(sentence)\n",
    "    \n",
    "    def normalize_camera_motion(self, motion_text):\n",
    "        \"\"\"Normalize camera motion text for consistent output\"\"\"\n",
    "        # Ensure starts with \"Camera\"\n",
    "        text = motion_text.strip()\n",
    "        \n",
    "        if text.lower().startswith(\"the camera\"):\n",
    "            text = \"Camera\" + text[10:]\n",
    "        elif text.lower().startswith(\"a camera\"):\n",
    "            text = \"Camera\" + text[8:]\n",
    "        elif not text.lower().startswith(\"camera\"):\n",
    "            text = \"Camera \" + text\n",
    "        \n",
    "        # Capitalize first letter\n",
    "        text = text[0].upper() + text[1:]\n",
    "        \n",
    "        # Remove ending punctuation\n",
    "        if text and text[-1] in '.,:;!?':\n",
    "            text = text[:-1]\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_camera_parts(self, text, camera_texts):\n",
    "        \"\"\"Remove camera motion descriptions from text\"\"\"\n",
    "        if not camera_texts:  # If no camera texts to remove, return as is\n",
    "            return text\n",
    "            \n",
    "        result = text\n",
    "        \n",
    "        for motion_text in camera_texts:\n",
    "            # Remove exact camera motion text\n",
    "            result = result.replace(motion_text, \"\").strip()\n",
    "            \n",
    "        # Clean up any residuals\n",
    "        result = RE_WHITESPACE.sub(' ', result).strip()\n",
    "        result = re.sub(r'^(?:as|while|when|and|then)\\s+', '', result).strip()\n",
    "        result = re.sub(r'^\\s*[,;:]\\s*', '', result).strip()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def identify_all_subjects(self, sentences, camera_text_to_remove):\n",
    "        \"\"\"First pass: identify all subjects across the entire text\"\"\"\n",
    "        # If we've already cached results, return them\n",
    "        if self.cached_subjects is not None and self.cached_subject_refs is not None:\n",
    "            return self.cached_subjects, self.cached_subject_refs\n",
    "        \n",
    "        subjects = {}  # Final subject dictionary\n",
    "        subject_refs = []  # Track all subject references\n",
    "        \n",
    "        # First, find all potential subject mentions\n",
    "        for sent_idx, sent in enumerate(sentences):\n",
    "            # Try to find subjects in both original and cleaned text\n",
    "            clean_sent = self.remove_camera_parts(sent, camera_text_to_remove)\n",
    "            texts_to_check = [sent, clean_sent] if clean_sent != sent else [sent]\n",
    "            \n",
    "            for text in texts_to_check:\n",
    "                if not text.strip():\n",
    "                    continue\n",
    "                    \n",
    "                # Skip sentences that purely describe scene/environment\n",
    "                if self.is_scene_description(text):\n",
    "                    continue\n",
    "                    \n",
    "                # Find subjects in this text\n",
    "                found_subjects = self.find_subjects_in_sentence(text)\n",
    "                \n",
    "                for subj in found_subjects:\n",
    "                    # Add sentence position\n",
    "                    subj[\"sentence_idx\"] = sent_idx\n",
    "                    \n",
    "                    # Check if this exact subject reference already exists\n",
    "                    if not any(existing[\"start_idx\"] == subj[\"start_idx\"] and \n",
    "                              existing[\"sentence_idx\"] == sent_idx for existing in subject_refs):\n",
    "                        subject_refs.append(subj)\n",
    "        \n",
    "        # Consolidate subjects (group references to same subject)\n",
    "        subject_groups = self.group_subject_references(subject_refs)\n",
    "        \n",
    "        # Create final subject dictionary\n",
    "        for i, refs in enumerate(subject_groups):\n",
    "            subj_id = f\"Subject {i+1}\"\n",
    "            base_ref = refs[0]  # Use first reference as base\n",
    "            \n",
    "            # Check if it's too generic a subject term to include\n",
    "            if base_ref[\"noun\"].lower() in [\"characters\", \"individuals\", \"subjects\"] and len(subject_groups) > 1:\n",
    "                # Skip this generic subject if we have other more specific ones\n",
    "                continue\n",
    "            \n",
    "            # Find most detailed attributes\n",
    "            attributes = self.get_best_attributes(refs)\n",
    "            \n",
    "            subjects[subj_id] = {\n",
    "                \"base_noun\": base_ref[\"noun\"],\n",
    "                \"attributes\": attributes,\n",
    "                \"actions\": [],\n",
    "                \"refs\": refs  # Keep all references for action extraction\n",
    "            }\n",
    "        \n",
    "        # Cache results for future use\n",
    "        self.cached_subjects = subjects\n",
    "        self.cached_subject_refs = subject_refs\n",
    "        \n",
    "        return subjects, subject_refs\n",
    "    \n",
    "    def identify_subjects_generic(self, sentences, camera_text_to_remove):\n",
    "        \"\"\"Identify subjects in a more generic way when specific identification fails\"\"\"\n",
    "        subjects = {}  # Final subject dictionary\n",
    "        subject_refs = []  # Track all subject references\n",
    "        \n",
    "        # Generic subjects based on pronouns and actions\n",
    "        pronoun_subjects = {\n",
    "            \"he/him\": {\"nouns\": [\"man\", \"boy\", \"male\"], \"pronouns\": [\"he\", \"him\", \"his\"]},\n",
    "            \"she/her\": {\"nouns\": [\"woman\", \"girl\", \"female\"], \"pronouns\": [\"she\", \"her\", \"hers\"]},\n",
    "            \"they/them\": {\"nouns\": [\"people\", \"individuals\", \"characters\"], \"pronouns\": [\"they\", \"them\", \"their\"]}\n",
    "        }\n",
    "        \n",
    "        # Try to find subjects in original sentences first, which may include camera parts\n",
    "        for sent_idx, sent in enumerate(sentences):\n",
    "            # Look for subjects that might appear after camera motion descriptions\n",
    "            for subj_type in SUBJECT_NOUNS:\n",
    "                # Create more specific search for color+clothing patterns\n",
    "                for color in COLOR_TERMS:\n",
    "                    for clothing in CLOTHING_TERMS:\n",
    "                        pattern = rf'\\b(?:a|an|the)\\s+{subj_type}\\s+in\\s+{color}\\s+{clothing}\\b'\n",
    "                        matches = list(re.finditer(pattern, sent, re.IGNORECASE))\n",
    "                        \n",
    "                        if matches:\n",
    "                            for match in matches:\n",
    "                                subj_id = f\"Subject {len(subjects)+1}\"\n",
    "                                \n",
    "                                # Create a subject entry with clear attributes\n",
    "                                subjects[subj_id] = {\n",
    "                                    \"base_noun\": subj_type,\n",
    "                                    \"attributes\": f\"in {color} {clothing}\",\n",
    "                                    \"actions\": [],\n",
    "                                    \"refs\": []\n",
    "                                }\n",
    "                                \n",
    "                                # Create a reference\n",
    "                                ref = {\n",
    "                                    \"noun\": subj_type,\n",
    "                                    \"full_description\": match.group(0),\n",
    "                                    \"attributes\": f\"in {color} {clothing}\",\n",
    "                                    \"start_idx\": match.start(),\n",
    "                                    \"end_idx\": match.end(),\n",
    "                                    \"sentence_idx\": sent_idx,\n",
    "                                    \"has_definite_article\": 'the' in match.group(0).lower()\n",
    "                                }\n",
    "                                \n",
    "                                subjects[subj_id][\"refs\"] = [ref]\n",
    "                                subject_refs.append(ref)\n",
    "        \n",
    "        # If we found subjects using specific patterns, return them\n",
    "        if subjects:\n",
    "            return subjects, subject_refs\n",
    "        \n",
    "        # Otherwise, proceed with generic pronoun-based identification\n",
    "        # First, look for pronouns to determine subject types\n",
    "        pronoun_counts = {k: 0 for k in pronoun_subjects.keys()}\n",
    "        for sent in sentences:\n",
    "            clean_sent = self.remove_camera_parts(sent, camera_text_to_remove)\n",
    "            if not clean_sent.strip():\n",
    "                continue\n",
    "            \n",
    "            for subj_type, info in pronoun_subjects.items():\n",
    "                for pronoun in info[\"pronouns\"]:\n",
    "                    pattern = r'\\b' + pronoun + r'\\b'\n",
    "                    matches = list(re.finditer(pattern, clean_sent, re.IGNORECASE))\n",
    "                    pronoun_counts[subj_type] += len(matches)\n",
    "        \n",
    "        # Create generic subjects based on pronouns found\n",
    "        subject_id = 1\n",
    "        for subj_type, count in sorted(pronoun_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            if count > 0:\n",
    "                info = pronoun_subjects[subj_type]\n",
    "                subj_id = f\"Subject {subject_id}\"\n",
    "                \n",
    "                # Use first noun as base noun\n",
    "                base_noun = info[\"nouns\"][0]\n",
    "                \n",
    "                # Create a subject entry\n",
    "                subjects[subj_id] = {\n",
    "                    \"base_noun\": base_noun,\n",
    "                    \"attributes\": \"\",  # Generic subjects have no specific attributes\n",
    "                    \"actions\": [],\n",
    "                    \"refs\": []  # No specific references\n",
    "                }\n",
    "                \n",
    "                # Create a reference for this subject\n",
    "                ref = {\n",
    "                    \"noun\": base_noun,\n",
    "                    \"full_description\": base_noun,\n",
    "                    \"attributes\": \"\",\n",
    "                    \"start_idx\": 0,  # Generic start index\n",
    "                    \"end_idx\": len(base_noun),  # Generic end index\n",
    "                    \"sentence_idx\": 0,  # Put in first sentence\n",
    "                    \"has_definite_article\": False\n",
    "                }\n",
    "                \n",
    "                subjects[subj_id][\"refs\"] = [ref]\n",
    "                subject_refs.append(ref)\n",
    "                \n",
    "                subject_id += 1\n",
    "        \n",
    "        # If no pronouns were found, create a generic \"person\" subject\n",
    "        if not subjects:\n",
    "            subj_id = \"Subject 1\"\n",
    "            base_noun = \"person\"\n",
    "            \n",
    "            subjects[subj_id] = {\n",
    "                \"base_noun\": base_noun,\n",
    "                \"attributes\": \"\",\n",
    "                \"actions\": [],\n",
    "                \"refs\": []\n",
    "            }\n",
    "            \n",
    "            ref = {\n",
    "                \"noun\": base_noun,\n",
    "                \"full_description\": base_noun,\n",
    "                \"attributes\": \"\",\n",
    "                \"start_idx\": 0,\n",
    "                \"end_idx\": len(base_noun),\n",
    "                \"sentence_idx\": 0,\n",
    "                \"has_definite_article\": False\n",
    "            }\n",
    "            \n",
    "            subjects[subj_id][\"refs\"] = [ref]\n",
    "            subject_refs.append(ref)\n",
    "        \n",
    "        return subjects, subject_refs\n",
    "    \n",
    "    def find_subjects_in_sentence(self, sentence):\n",
    "        \"\"\"Find all subject mentions in a sentence with improved attribute extraction\"\"\"\n",
    "        subjects = []\n",
    "        \n",
    "        # Optimization: cache lowercase version of sentence\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        # Efficient pre-check - if doesn't contain any subject terms at all, return immediately\n",
    "        if not any(noun.lower() in sentence_lower for noun in SUBJECT_NOUNS_SET):\n",
    "            return subjects\n",
    "        \n",
    "        # Sorted by length (longer first) to capture \"tennis player\" before \"player\" etc.\n",
    "        subject_nouns = SUBJECT_NOUNS\n",
    "        \n",
    "        # Create pattern cache\n",
    "        if not hasattr(self, 'subject_pattern_cache'):\n",
    "            self.subject_pattern_cache = {}\n",
    "        \n",
    "        # First look for subjects with specific color descriptions (e.g., \"man in cyan clothing\")\n",
    "        for noun in subject_nouns:\n",
    "            if noun not in sentence_lower:  # Quick check\n",
    "                continue\n",
    "                \n",
    "            for color in COLOR_TERMS:\n",
    "                if color not in sentence_lower:  # Quick check\n",
    "                    continue\n",
    "                    \n",
    "                # Pattern for \"X in [color] clothing/clothes/outfit\"\n",
    "                color_pattern_key = f\"color_{noun}_{color}\"\n",
    "                if color_pattern_key not in self.subject_pattern_cache:\n",
    "                    pattern = rf'\\b(?:A|An|The|a|an|the)?\\s*{noun}\\s+in\\s+{color}\\s+(?:clothing|clothes|outfit|shirt|dress|suit|jacket|pants|top)\\b'\n",
    "                    self.subject_pattern_cache[color_pattern_key] = re.compile(pattern, re.IGNORECASE)\n",
    "                \n",
    "                for match in self.subject_pattern_cache[color_pattern_key].finditer(sentence):\n",
    "                    if not self.is_part_of_existing_subject(match.start(), subjects):\n",
    "                        self.extract_subject(match, noun, sentence, subjects, \n",
    "                                        'the' in match.group().lower() or \n",
    "                                        'this' in match.group().lower() or \n",
    "                                        'that' in match.group().lower())\n",
    "                \n",
    "                # Pattern for \"[color]-clothed X\" or \"[color] X\" \n",
    "                color_adj_key = f\"color_adj_{noun}_{color}\"\n",
    "                if color_adj_key not in self.subject_pattern_cache:\n",
    "                    pattern = rf'\\b(?:A|An|The|a|an|the)?\\s*{color}(?:-clothed|-dressed)?\\s+{noun}\\b'\n",
    "                    self.subject_pattern_cache[color_adj_key] = re.compile(pattern, re.IGNORECASE)\n",
    "                \n",
    "                for match in self.subject_pattern_cache[color_adj_key].finditer(sentence):\n",
    "                    if not self.is_part_of_existing_subject(match.start(), subjects):\n",
    "                        self.extract_subject(match, noun, sentence, subjects, \n",
    "                                        'the' in match.group().lower() or \n",
    "                                        'this' in match.group().lower() or \n",
    "                                        'that' in match.group().lower())\n",
    "        \n",
    "        # Patterns for finding subjects with various determiners and adjectives\n",
    "        for noun in subject_nouns:\n",
    "            if noun not in sentence_lower:  # Quick check\n",
    "                continue\n",
    "                \n",
    "            # Pattern for \"a/an/the [adjectives] noun\"\n",
    "            det_key = f\"det_{noun}\"\n",
    "            if det_key not in self.subject_pattern_cache:\n",
    "                pattern = rf'\\b(?:A|An|The|a|an|the)\\s+(?:[a-z]+[-\\s]+)*{noun}\\b'\n",
    "                self.subject_pattern_cache[det_key] = re.compile(pattern, re.IGNORECASE)\n",
    "            \n",
    "            for match in self.subject_pattern_cache[det_key].finditer(sentence):\n",
    "                if not self.is_part_of_existing_subject(match.start(), subjects):\n",
    "                    self.extract_subject(match, noun, sentence, subjects, \n",
    "                                    'the' in match.group().lower() or \n",
    "                                    'this' in match.group().lower() or \n",
    "                                    'that' in match.group().lower())\n",
    "            \n",
    "            # Pattern for \"this/that/these/those [adjectives] noun\"\n",
    "            demo_key = f\"demo_{noun}\"\n",
    "            if demo_key not in self.subject_pattern_cache:\n",
    "                pattern = rf'\\b(?:This|That|These|Those|this|that|these|those)\\s+(?:[a-z]+[-\\s]+)*{noun}\\b'\n",
    "                self.subject_pattern_cache[demo_key] = re.compile(pattern, re.IGNORECASE)\n",
    "            \n",
    "            for match in self.subject_pattern_cache[demo_key].finditer(sentence):\n",
    "                if not self.is_part_of_existing_subject(match.start(), subjects):\n",
    "                    self.extract_subject(match, noun, sentence, subjects, \n",
    "                                    'the' in match.group().lower() or \n",
    "                                    'this' in match.group().lower() or \n",
    "                                    'that' in match.group().lower())\n",
    "            \n",
    "            # Pattern for adj + noun without determiners\n",
    "            adj_key = f\"adj_{noun}\"\n",
    "            if adj_key not in self.subject_pattern_cache:\n",
    "                pattern = rf'\\b(?:[A-Z][a-z]+\\s+)+{noun}\\b'\n",
    "                self.subject_pattern_cache[adj_key] = re.compile(pattern, re.IGNORECASE)\n",
    "            \n",
    "            for match in self.subject_pattern_cache[adj_key].finditer(sentence):\n",
    "                if not self.is_part_of_existing_subject(match.start(), subjects):\n",
    "                    self.extract_subject(match, noun, sentence, subjects, \n",
    "                                    'the' in match.group().lower() or \n",
    "                                    'this' in match.group().lower() or \n",
    "                                    'that' in match.group().lower())\n",
    "            \n",
    "            # Pattern for just the noun with capital (likely at sentence beginning)\n",
    "            noun_key = f\"noun_{noun}\"\n",
    "            if noun_key not in self.subject_pattern_cache:\n",
    "                pattern = rf'\\b{noun}\\b'\n",
    "                self.subject_pattern_cache[noun_key] = re.compile(pattern, re.IGNORECASE)\n",
    "            \n",
    "            for match in self.subject_pattern_cache[noun_key].finditer(sentence):\n",
    "                # Make sure noun is not just a common word in middle of sentence\n",
    "                if match.start() > 0 and sentence[match.start()-1] not in \" \\t\\n.,;:?!()[]{}\\\"'\":\n",
    "                    continue\n",
    "                \n",
    "                if not self.is_part_of_existing_subject(match.start(), subjects):\n",
    "                    self.extract_subject(match, noun, sentence, subjects, \n",
    "                                    'the' in match.group().lower() or \n",
    "                                    'this' in match.group().lower() or \n",
    "                                    'that' in match.group().lower())\n",
    "        \n",
    "        # Sort by position in sentence\n",
    "        subjects.sort(key=lambda x: x[\"start_idx\"])\n",
    "        return subjects\n",
    "    \n",
    "    def is_part_of_existing_subject(self, pos, subjects):\n",
    "        \"\"\"Check if position is within an existing subject's range\"\"\"\n",
    "        return any(s[\"start_idx\"] <= pos < s[\"end_idx\"] for s in subjects)\n",
    "    \n",
    "    def extract_subject(self, match, noun, text, subjects, has_definite_article):\n",
    "        \"\"\"Extract a subject and its attributes\"\"\"\n",
    "        start_idx = match.start()\n",
    "        \n",
    "        # Extract complete subject phrase\n",
    "        full_desc, end_idx = self.extract_complete_subject(text, start_idx, noun)\n",
    "        \n",
    "        # Extract attributes\n",
    "        attributes = self.extract_subject_attributes(full_desc, noun)\n",
    "        \n",
    "        subjects.append({\n",
    "            \"noun\": noun,\n",
    "            \"full_description\": full_desc,\n",
    "            \"attributes\": attributes,\n",
    "            \"start_idx\": start_idx,\n",
    "            \"end_idx\": end_idx,\n",
    "            \"has_definite_article\": has_definite_article\n",
    "        })\n",
    "    \n",
    "    def extract_complete_subject(self, text, start_idx, noun):\n",
    "        \"\"\"Extract complete subject phrase including attributes\"\"\"\n",
    "        # Find end of basic noun phrase\n",
    "        noun_pos = text[start_idx:].lower().find(noun.lower())\n",
    "        if noun_pos == -1:  # Safety check\n",
    "            return text[start_idx:start_idx + len(noun)].strip(), start_idx + len(noun)\n",
    "            \n",
    "        basic_end = start_idx + noun_pos + len(noun)\n",
    "        \n",
    "        # Look for additional attributes after the noun\n",
    "        remaining = text[basic_end:].lstrip()\n",
    "        \n",
    "        # Common attribute markers\n",
    "        attr_markers = [\n",
    "            r'\\bwith\\b', r'\\bin\\b', r'\\bwearing\\b', r'\\bholding\\b', r'\\bcarrying\\b',\n",
    "            r'\\bwho\\s+is\\b', r'\\bdressed\\s+in\\b', r'\\bhaving\\b', r'\\bclad\\s+in\\b',\n",
    "            r'\\bequipped\\s+with\\b', r'\\bfeaturing\\b', r'\\bdisplaying\\b'\n",
    "        ]\n",
    "        \n",
    "        # Check for attributes\n",
    "        full_subject = text[start_idx:basic_end]\n",
    "        current_end = basic_end\n",
    "        \n",
    "        # First check if next content is action rather than attribute\n",
    "        if re.match(r'^\\s+(?:is|was|are|were)\\s+(?:walking|running|jumping|sitting|standing|lying|looking|reaching)', remaining):\n",
    "            return full_subject.strip(), current_end\n",
    "        \n",
    "        # Look for attribute markers\n",
    "        for marker in attr_markers:\n",
    "            marker_match = re.search(f\"^\\\\s*{marker}\\\\s+\", remaining)\n",
    "            if marker_match:\n",
    "                # Find reasonable ending for this attribute phrase\n",
    "                attr_end = self.find_attribute_boundary(remaining)\n",
    "                \n",
    "                if attr_end > 0:\n",
    "                    # Add this attribute to subject phrase\n",
    "                    attr_phrase = remaining[:attr_end].strip()\n",
    "                    full_subject += \" \" + attr_phrase\n",
    "                    current_end += len(attr_phrase) + 1  # +1 for space\n",
    "                    \n",
    "                    # Update remaining text and continue checking more attributes\n",
    "                    remaining = remaining[attr_end:].lstrip()\n",
    "                else:\n",
    "                    # If no clear ending, use next punctuation\n",
    "                    punct_match = re.search(r'[.,;:]', remaining)\n",
    "                    if punct_match:\n",
    "                        attr_phrase = remaining[:punct_match.start()].strip()\n",
    "                        full_subject += \" \" + attr_phrase\n",
    "                        current_end += len(attr_phrase) + 1\n",
    "        \n",
    "        # Look for \"who\" or \"that\" relative clauses describing subject\n",
    "        relative_clause_match = re.search(r'^\\s+(?:who|that|which)\\s+(?:is|are|was|were)\\s+(?!walking|running|jumping|sitting|standing)', remaining)\n",
    "        if relative_clause_match:\n",
    "            # Find end of relative clause\n",
    "            rel_clause_end = self.find_attribute_boundary(remaining)\n",
    "            if rel_clause_end > 0:\n",
    "                rel_clause = remaining[:rel_clause_end].strip()\n",
    "                full_subject += \" \" + rel_clause\n",
    "                current_end += len(rel_clause) + 1\n",
    "        \n",
    "        return full_subject.strip(), current_end\n",
    "    \n",
    "    def find_attribute_boundary(self, text):\n",
    "        \"\"\"Find end position of attribute phrase before action begins\"\"\"\n",
    "        # Look for punctuation\n",
    "        punct_match = re.search(r'[.,;:]', text)\n",
    "        punct_pos = punct_match.start() if punct_match else len(text)\n",
    "        \n",
    "        # Look for verbs\n",
    "        verb_pos = len(text)\n",
    "        for pattern in self.patterns['verb_patterns']:\n",
    "            verb_match = pattern.search(text)\n",
    "            if verb_match and verb_match.start() < verb_pos:\n",
    "                verb_pos = verb_match.start()\n",
    "                break  # One is enough\n",
    "        \n",
    "        # Look for connectors\n",
    "        connector_pos = len(text)\n",
    "        connector_patterns = [r'\\band\\b', r'\\bwhile\\b', r'\\bthen\\b', r'\\bas\\b', r'\\bbut\\b', r'\\bor\\b']\n",
    "        \n",
    "        for pattern in connector_patterns:\n",
    "            conn_match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if conn_match and conn_match.start() < connector_pos:\n",
    "                connector_pos = conn_match.start()\n",
    "                break  # One is enough\n",
    "        \n",
    "        # Return earliest boundary\n",
    "        return min(punct_pos, verb_pos, connector_pos)\n",
    "    \n",
    "    def extract_subject_attributes(self, description, noun):\n",
    "        \"\"\"Extract clean attributes from subject description, focusing on physical appearance and held items\"\"\"\n",
    "        # Clean up description\n",
    "        desc = description.lower().strip()\n",
    "        \n",
    "        # Remove common articles at start\n",
    "        for article in ['the ', 'a ', 'an ']:\n",
    "            if desc.startswith(article):\n",
    "                desc = desc[len(article):]\n",
    "                break\n",
    "        \n",
    "        # Filter out scene-related information\n",
    "        scene_indicators = [\n",
    "            \"video sequence begins with\", \"video shows\", \"video begins with\",\n",
    "            \"the scene shows\", \"in the scene\", \"in the video\", \"in the frame\",\n",
    "            \"appears to be\", \"what appears to be\"\n",
    "        ]\n",
    "        \n",
    "        for indicator in scene_indicators:\n",
    "            if indicator in desc:\n",
    "                desc = desc.replace(indicator, \"\").strip()\n",
    "        \n",
    "        # Extract attributes based on specific patterns\n",
    "        attributes = []\n",
    "        \n",
    "        # 1. Extract adjectives and descriptors before the noun\n",
    "        pre_noun_pattern = rf'(.*?)\\b{re.escape(noun.lower())}\\b'\n",
    "        pre_noun_match = re.search(pre_noun_pattern, desc)\n",
    "        \n",
    "        if pre_noun_match:\n",
    "            pre_noun_text = pre_noun_match.group(1).strip()\n",
    "            # Only consider last few words as potential adjectives\n",
    "            words = pre_noun_text.split()\n",
    "            \n",
    "            if words:\n",
    "                # Look at last 3 words max, which are likely adjectives\n",
    "                potential_adjectives = words[-3:] if len(words) >= 3 else words\n",
    "                \n",
    "                # Filter out common determiners, keep descriptive adjectives\n",
    "                descriptive_words = []\n",
    "                for word in potential_adjectives:\n",
    "                    if word.lower() not in ['the', 'a', 'an', 'this', 'that', 'these', 'those', 'and', 'or']:\n",
    "                        descriptive_words.append(word)\n",
    "                \n",
    "                if descriptive_words:\n",
    "                    attributes.append(\" \".join(descriptive_words))\n",
    "        \n",
    "        # 2. Look for key attribute phrases after the noun\n",
    "        # Use precompiled attribute patterns\n",
    "        for pattern, formatter in self.patterns['attribute_patterns']:\n",
    "            matches = pattern.finditer(desc)\n",
    "            for match in matches:\n",
    "                formatted_attr = formatter(match)\n",
    "                \n",
    "                # Make sure this attribute doesn't contain action verbs\n",
    "                if not self.contains_action_verb(formatted_attr):\n",
    "                    attributes.append(formatted_attr)\n",
    "        \n",
    "        # Combine all attributes and remove duplicates\n",
    "        combined_attributes = \", \".join(filter(None, attributes))\n",
    "        \n",
    "        # Remove duplicate attributes \n",
    "        parts = [p.strip() for p in combined_attributes.split(',')]\n",
    "        unique_parts = []\n",
    "        for part in parts:\n",
    "            normalized_part = self.normalize_attribute(part)\n",
    "            if normalized_part and not any(self.normalize_attribute(up) == normalized_part for up in unique_parts):\n",
    "                unique_parts.append(part)\n",
    "            \n",
    "        return \", \".join(unique_parts)\n",
    "    \n",
    "    def contains_action_verb(self, text):\n",
    "        \"\"\"Check if text contains common action verbs that shouldn't be in attributes\"\"\"\n",
    "        # Cache check results\n",
    "        cache_key = f\"action_verb_{text}\"\n",
    "        if cache_key in self.subject_lookup_cache:\n",
    "            return self.subject_lookup_cache[cache_key]\n",
    "            \n",
    "        action_verbs = [\n",
    "            'walks', 'runs', 'jumps', 'sits', 'stands', 'moves', 'turns',\n",
    "            'looks', 'stares', 'gazes', 'speaks', 'talks', 'says',\n",
    "            'reaches', 'grabs', 'takes', 'puts', 'places', 'throws',\n",
    "            'gestures', 'smiles', 'frowns', 'laughs', 'cries'\n",
    "        ]\n",
    "        \n",
    "        # Check for these verbs (ensure they're complete words)\n",
    "        for verb in action_verbs:\n",
    "            if re.search(rf'\\b{verb}\\b', text.lower()):\n",
    "                self.subject_lookup_cache[cache_key] = True\n",
    "                return True\n",
    "        \n",
    "        # Also check for -ing forms as verbs (non-descriptive -ing forms)\n",
    "        ing_verbs = ['walking', 'running', 'jumping', 'sitting', 'standing',\n",
    "                    'looking', 'speaking', 'talking',\n",
    "                    'reaching', 'grabbing', 'taking', 'putting', 'placing',\n",
    "                    'gesturing', 'smiling', 'frowning', 'laughing', 'crying']\n",
    "        \n",
    "        # Only treat these as action verbs when they're not modified by determiners\n",
    "        for verb in ing_verbs:\n",
    "            # Check if verb exists\n",
    "            match = re.search(rf'\\b{verb}\\b', text.lower())\n",
    "            if match:\n",
    "                # Check if it has determiners before it (a, an, the)\n",
    "                start = match.start()\n",
    "                prefix = text.lower()[max(0, start-4):start]\n",
    "                # If NOT preceded by determiner (which would make it a noun)\n",
    "                if not (prefix.endswith('the ') or prefix.endswith('a ') or prefix.endswith('an ')):\n",
    "                    self.subject_lookup_cache[cache_key] = True\n",
    "                    return True\n",
    "        \n",
    "        self.subject_lookup_cache[cache_key] = False\n",
    "        return False\n",
    "    \n",
    "    def normalize_attribute(self, attribute):\n",
    "        \"\"\"Normalize attribute string to help detect duplicates\"\"\"\n",
    "        if not attribute:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        normalized = attribute.lower().strip()\n",
    "        \n",
    "        # Remove unnecessary starting words\n",
    "        prefixes = [\"with \", \"in \", \"having \", \"featuring \", \"displaying \"]\n",
    "        for prefix in prefixes:\n",
    "            if normalized.startswith(prefix):\n",
    "                normalized = normalized[len(prefix):].strip()\n",
    "                break\n",
    "        \n",
    "        # Simplify common clothing-related phrases\n",
    "        clothing_simplifications = {\n",
    "            \"dressed in a\": \"wearing a\",\n",
    "            \"dressed in the\": \"wearing the\",\n",
    "            \"clad in a\": \"wearing a\",\n",
    "            \"clad in the\": \"wearing the\"\n",
    "        }\n",
    "        \n",
    "        for phrase, replacement in clothing_simplifications.items():\n",
    "            if normalized.startswith(phrase):\n",
    "                normalized = normalized.replace(phrase, replacement, 1)\n",
    "        \n",
    "        return normalized.strip()\n",
    "    \n",
    "    def get_best_attributes(self, refs):\n",
    "        \"\"\"Get the best, most descriptive attributes from a set of references, avoiding duplicates\"\"\"\n",
    "        # First, check for references with clothing/appearance descriptions (wearing, holding etc.)\n",
    "        appearance_refs = [ref for ref in refs if \n",
    "                          any(word in ref[\"attributes\"].lower() for word in \n",
    "                             [\"wearing\", \"dressed\", \"holding\", \"carrying\", \"with\", \"glasses\", \"shirt\", \"hat\"])]\n",
    "        \n",
    "        if appearance_refs:\n",
    "            # Sort by attribute length and take longest\n",
    "            appearance_refs.sort(key=lambda r: len(r[\"attributes\"]), reverse=True)\n",
    "            attributes = appearance_refs[0][\"attributes\"]\n",
    "            # Make sure not to return duplicate attributes\n",
    "            parts = [p.strip() for p in attributes.split(',')]\n",
    "            unique_parts = []\n",
    "            for part in parts:\n",
    "                normalized = self.normalize_attribute(part)\n",
    "                if part and normalized and not any(self.normalize_attribute(up) == normalized for up in unique_parts):\n",
    "                    unique_parts.append(part)\n",
    "            return \", \".join(unique_parts)\n",
    "        \n",
    "        # Next check for references with color or age\n",
    "        descriptive_refs = [ref for ref in refs if \n",
    "                          any(word in ref[\"attributes\"].lower() for word in COLOR_TERMS + \n",
    "                             [\"young\", \"old\", \"elderly\", \"middle-aged\"])]\n",
    "        \n",
    "        if descriptive_refs:\n",
    "            # Sort by attribute length and take longest\n",
    "            descriptive_refs.sort(key=lambda r: len(r[\"attributes\"]), reverse=True)\n",
    "            attributes = descriptive_refs[0][\"attributes\"]\n",
    "            # Make sure not to return duplicate attributes\n",
    "            parts = [p.strip() for p in attributes.split(',')]\n",
    "            unique_parts = []\n",
    "            for part in parts:\n",
    "                normalized = self.normalize_attribute(part)\n",
    "                if part and normalized and not any(self.normalize_attribute(up) == normalized for up in unique_parts):\n",
    "                    unique_parts.append(part)\n",
    "            return \", \".join(unique_parts)\n",
    "        \n",
    "        # Otherwise, take longest attribute string\n",
    "        refs_with_attrs = [ref for ref in refs if ref[\"attributes\"]]\n",
    "        if refs_with_attrs:\n",
    "            refs_with_attrs.sort(key=lambda r: len(r[\"attributes\"]), reverse=True)\n",
    "            attributes = refs_with_attrs[0][\"attributes\"]\n",
    "            # Make sure not to return duplicate attributes\n",
    "            parts = [p.strip() for p in attributes.split(',')]\n",
    "            unique_parts = []\n",
    "            for part in parts:\n",
    "                normalized = self.normalize_attribute(part)\n",
    "                if part and normalized and not any(self.normalize_attribute(up) == normalized for up in unique_parts):\n",
    "                    unique_parts.append(part)\n",
    "            return \", \".join(unique_parts)\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def group_subject_references(self, subject_refs):\n",
    "        \"\"\"Group references that likely refer to the same subject\"\"\"\n",
    "        if not subject_refs:\n",
    "            return []\n",
    "            \n",
    "        groups = []\n",
    "        processed = set()\n",
    "        \n",
    "        # First pass - group by specific attributes (like color) to avoid merging different subjects\n",
    "        for i, ref in enumerate(subject_refs):\n",
    "            if i in processed:\n",
    "                continue\n",
    "                \n",
    "            # Start a new group with this reference\n",
    "            current_group = [ref]\n",
    "            processed.add(i)\n",
    "            \n",
    "            # Only find references that clearly match this one (strict matching)\n",
    "            for j, other_ref in enumerate(subject_refs):\n",
    "                if j in processed or j == i:\n",
    "                    continue\n",
    "                    \n",
    "                # Only group if they have same distinguishing attributes\n",
    "                if self.references_same_subject_strict(ref, other_ref):\n",
    "                    current_group.append(other_ref)\n",
    "                    processed.add(j)\n",
    "            \n",
    "            groups.append(current_group)\n",
    "        \n",
    "        return groups\n",
    "    \n",
    "    def references_same_subject_strict(self, ref1, ref2):\n",
    "        \"\"\"Determine if two references clearly refer to same subject, improved matching\"\"\"\n",
    "        # Check if nouns are same or synonyms\n",
    "        if ref1[\"noun\"] != ref2[\"noun\"]:\n",
    "            # Check synonyms and compound terms\n",
    "            same_meaning = False\n",
    "            \n",
    "            # Use predefined synonym dictionary\n",
    "            for base_noun, synonyms in SUBJECT_SYNONYMS.items():\n",
    "                # Check if both nouns relate to base noun or its synonyms\n",
    "                if ((ref1[\"noun\"] == base_noun or ref1[\"noun\"] in synonyms) and\n",
    "                    (ref2[\"noun\"] == base_noun or ref2[\"noun\"] in synonyms)):\n",
    "                    same_meaning = True\n",
    "                    break\n",
    "            \n",
    "            # Check if one is compound word containing other (e.g., \"tennis player\" and \"player\")\n",
    "            if not same_meaning:\n",
    "                if (ref1[\"noun\"] in ref2[\"noun\"] and ref2[\"noun\"].endswith(ref1[\"noun\"])) or \\\n",
    "                   (ref2[\"noun\"] in ref1[\"noun\"] and ref1[\"noun\"].endswith(ref2[\"noun\"])):\n",
    "                    same_meaning = True\n",
    "            \n",
    "            if not same_meaning:\n",
    "                return False\n",
    "        \n",
    "        # Extract words from attributes - use sets instead of re.findall for efficiency\n",
    "        attr1 = ref1[\"attributes\"].lower()\n",
    "        attr2 = ref2[\"attributes\"].lower()\n",
    "        \n",
    "        # Check color words\n",
    "        color_words1 = {color for color in COLOR_TERMS if f\" {color} \" in f\" {attr1} \" or attr1.startswith(f\"{color} \")}\n",
    "        color_words2 = {color for color in COLOR_TERMS if f\" {color} \" in f\" {attr2} \" or attr2.startswith(f\"{color} \")}\n",
    "        \n",
    "        # If both have colors and they differ, they're different subjects\n",
    "        if color_words1 and color_words2 and not color_words1.intersection(color_words2):\n",
    "            return False\n",
    "        \n",
    "        # Check clothing descriptors with different colors\n",
    "        clothing1 = {clothing for clothing in CLOTHING_TERMS if f\" {clothing} \" in f\" {attr1} \" or attr1.endswith(f\" {clothing}\")}\n",
    "        clothing2 = {clothing for clothing in CLOTHING_TERMS if f\" {clothing} \" in f\" {attr2} \" or attr2.endswith(f\" {clothing}\")}\n",
    "        \n",
    "        if color_words1 and clothing1 and color_words2 and clothing2:\n",
    "            # Different colored clothing indicates different subjects\n",
    "            return color_words1 == color_words2\n",
    "        \n",
    "        # If one has \"in [color] clothing\" and other doesn't mention color \n",
    "        # or mentions different color, they're different subjects\n",
    "        if ((\"in\" in attr1 and color_words1 and clothing1) or \n",
    "            (\"in\" in attr2 and color_words2 and clothing2)):\n",
    "            if color_words1 != color_words2:\n",
    "                return False\n",
    "        \n",
    "        # In other cases, use more relaxed matching\n",
    "        return self.attributes_are_consistent(ref1[\"attributes\"], ref2[\"attributes\"])\n",
    "    \n",
    "    def attributes_are_consistent(self, attrs1, attrs2):\n",
    "        \"\"\"Check if two sets of attributes are consistent (not contradictory)\"\"\"\n",
    "        # Extract key words from both attribute sets\n",
    "        stopwords = {'in', 'with', 'a', 'an', 'the', 'and', 'of', 'on', 'at', 'by', 'to', 'for', 'who'}\n",
    "        \n",
    "        # More efficient word extraction - not using regex\n",
    "        words1 = {word.strip(\" ,.;:\") for word in attrs1.lower().split() if word.strip(\" ,.;:\") not in stopwords}\n",
    "        words2 = {word.strip(\" ,.;:\") for word in attrs2.lower().split() if word.strip(\" ,.;:\") not in stopwords}\n",
    "        \n",
    "        # Define key attribute categories\n",
    "        key_categories = {\n",
    "            'colors': set(COLOR_TERMS),\n",
    "            'age': {'young', 'old', 'older', 'elderly', 'middle-aged', 'teenage', 'adult'},\n",
    "            'hair': {'black-haired', 'blonde', 'white-haired', 'gray-haired', 'red-haired', 'bald'},\n",
    "            'clothing': set(CLOTHING_TERMS)\n",
    "        }\n",
    "        \n",
    "        # Check for contradictions in key categories\n",
    "        for category, terms in key_categories.items():\n",
    "            terms1 = words1.intersection(terms)\n",
    "            terms2 = words2.intersection(terms)\n",
    "            \n",
    "            # If both have terms in this category and there's no overlap, they're contradictory\n",
    "            if terms1 and terms2 and not terms1.intersection(terms2):\n",
    "                return False\n",
    "        \n",
    "        # If no contradictions found, they're consistent\n",
    "        return True\n",
    "    \n",
    "    def is_scene_description(self, sentence):\n",
    "        \"\"\"Check if sentence primarily describes scene/environment rather than actions\"\"\"\n",
    "        # Use precompiled patterns\n",
    "        for pattern in self.patterns['scene_patterns']:\n",
    "            if pattern.search(sentence):\n",
    "                # Only mark as scene description if it doesn't contain clear action verbs\n",
    "                return not self.contains_clear_action(sentence)\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def contains_clear_action(self, text):\n",
    "        \"\"\"Check if text contains clear action verbs (not just state/descriptive verbs)\"\"\"\n",
    "        cache_key = f\"clear_action_{text[:50]}\"\n",
    "        if cache_key in self.action_extraction_cache:\n",
    "            return self.action_extraction_cache[cache_key]\n",
    "            \n",
    "        action_verbs = [\n",
    "            'walk', 'run', 'jump', 'pick', 'grab', 'take', 'put', 'place', 'throw', \n",
    "            'push', 'pull', 'lift', 'turn', 'twist', 'open', 'close', 'hold', 'shake',\n",
    "            'wave', 'point', 'raise', 'lower', 'enter', 'exit', 'stand', 'sit', 'move',\n",
    "            'serve', 'toss', 'hit', 'strike', 'swing', 'prepare', 'position', 'adjust',\n",
    "            'touch', 'reach', 'carry', 'bring', 'approach', 'step', 'pass', 'deliver',\n",
    "            'focus', 'concentrate', 'aim', 'play', 'perform', 'dance', 'spin', 'twist',\n",
    "            'bend', 'stretch', 'lean', 'bite', 'eat', 'drink', 'sip', 'swallow',\n",
    "            'search', 'interact'\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for verb in action_verbs:\n",
    "            # Check various forms of verb\n",
    "            if (f\" {verb} \" in f\" {text_lower} \" or \n",
    "                f\" {verb}s \" in f\" {text_lower} \" or \n",
    "                f\" {verb}ed \" in f\" {text_lower} \" or \n",
    "                f\" {verb}ing \" in f\" {text_lower} \"):\n",
    "                # Make sure it's not part of a nominal compound\n",
    "                if not re.search(rf'\\b(?:the|a|an)\\s+{verb}ing', text_lower):\n",
    "                    self.action_extraction_cache[cache_key] = True\n",
    "                    return True\n",
    "        \n",
    "        self.action_extraction_cache[cache_key] = False\n",
    "        return False\n",
    "    \n",
    "    def contains_potential_subject(self, text):\n",
    "        \"\"\"Check if text might contain a new subject\"\"\"\n",
    "        # Use precompiled patterns\n",
    "        for pattern in self.patterns['subject_indicators']:\n",
    "            if pattern.search(text):\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def contains_pronoun(self, text):\n",
    "        \"\"\"Check if text contains pronouns\"\"\"\n",
    "        # Cache common check results\n",
    "        cache_key = f\"pronoun_{text[:50]}\"\n",
    "        if cache_key in self.pronoun_cache:\n",
    "            return self.pronoun_cache[cache_key]\n",
    "            \n",
    "        text_lower = f\" {text.lower()} \"\n",
    "        pronouns = [' he ', ' she ', ' they ', ' it ', ' him ', ' her ', ' them ']\n",
    "        has_pronoun = any(p in text_lower for p in pronouns)\n",
    "        \n",
    "        self.pronoun_cache[cache_key] = has_pronoun\n",
    "        return has_pronoun\n",
    "    \n",
    "    def find_subject_for_reference(self, ref, subjects):\n",
    "        \"\"\"Find which subject a reference belongs to\"\"\"\n",
    "        cache_key = f\"subj_ref_{ref['sentence_idx']}_{ref['start_idx']}\"\n",
    "        if cache_key in self.subject_lookup_cache:\n",
    "            return self.subject_lookup_cache[cache_key]\n",
    "            \n",
    "        for subj_id, info in subjects.items():\n",
    "            if any(r[\"start_idx\"] == ref[\"start_idx\"] and \n",
    "                   r[\"sentence_idx\"] == ref[\"sentence_idx\"] \n",
    "                   for r in info[\"refs\"]):\n",
    "                self.subject_lookup_cache[cache_key] = subj_id\n",
    "                return subj_id\n",
    "                \n",
    "        self.subject_lookup_cache[cache_key] = None\n",
    "        return None\n",
    "    \n",
    "    def extract_subject_actions(self, sentence, subject_ref, all_subj_refs=None):\n",
    "        \"\"\"Extract actions related to a specific subject, avoiding actions by other subjects\"\"\"\n",
    "        start_idx = subject_ref[\"end_idx\"]\n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "        # Determine range of possible actions for this subject (until next subject or end)\n",
    "        end_idx = sentence_length\n",
    "        \n",
    "        if all_subj_refs:\n",
    "            # Find next subject in sentence\n",
    "            for other_ref in all_subj_refs:\n",
    "                if other_ref[\"start_idx\"] > start_idx and other_ref[\"start_idx\"] < end_idx:\n",
    "                    end_idx = other_ref[\"start_idx\"]\n",
    "        \n",
    "        # Extract only text between current subject and next subject as potential actions\n",
    "        action_text = sentence[start_idx:end_idx].strip()\n",
    "        \n",
    "        # Clean up text\n",
    "        if action_text.startswith(',') or action_text.startswith('.'):\n",
    "            action_text = action_text[1:].strip()\n",
    "        \n",
    "        # Remove leading conjunctions\n",
    "        action_text = re.sub(r'^(?:and|then|also)\\s+', '', action_text)\n",
    "        \n",
    "        if not action_text:\n",
    "            return []\n",
    "        \n",
    "        # Handle potential \"while\" clauses that might introduce new subjects\n",
    "        while_parts = action_text.split(\" while \", 1)\n",
    "        if len(while_parts) > 1 and self.contains_potential_subject(while_parts[1]):\n",
    "            # Only process part before \"while\"\n",
    "            action_text = while_parts[0].strip()\n",
    "        \n",
    "        # Handle \"as\" clauses that might contain subject actions\n",
    "        as_parts = action_text.split(\" as \", 1)\n",
    "        as_clause_actions = []\n",
    "        if len(as_parts) > 1:\n",
    "            # Process main part\n",
    "            main_action_text = as_parts[0].strip()\n",
    "            \n",
    "            # Check if \"as\" clause contains actions by current subject\n",
    "            as_clause = as_parts[1].strip()\n",
    "            if as_clause.startswith(('he ', 'she ', 'they ')):\n",
    "                # Extract actions from \"as\" clause\n",
    "                as_clause_actions = self.extract_clause_actions(as_clause, \" as \")\n",
    "        else:\n",
    "            main_action_text = action_text\n",
    "        \n",
    "        # Split text into separate actions (by conjunctions and commas)\n",
    "        action_parts = re.split(r'\\s+and\\s+|\\s*,\\s*|\\s*;\\s*', main_action_text)\n",
    "        actions = []\n",
    "        \n",
    "        for part in action_parts:\n",
    "            part = part.strip()\n",
    "            if part and self.contains_verb(part):\n",
    "                actions.append(part)\n",
    "        \n",
    "        # Add actions extracted from \"as\" clause\n",
    "        actions.extend(as_clause_actions)\n",
    "        \n",
    "        # Try to capture additional reactions and state changes\n",
    "        for pattern, formatter in self.patterns['state_patterns']:\n",
    "            matches = list(pattern.finditer(action_text))\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    state_desc = formatter(match)\n",
    "                    if state_desc not in actions:\n",
    "                        actions.append(state_desc)\n",
    "        \n",
    "        # Capture \"reacting to\" patterns\n",
    "        for pattern, formatter in self.patterns['reaction_patterns']:\n",
    "            matches = list(pattern.finditer(action_text))\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    reaction_desc = formatter(match)\n",
    "                    if reaction_desc not in actions:\n",
    "                        actions.append(reaction_desc)\n",
    "        \n",
    "        # If no actions found, try more comprehensive action extraction \n",
    "        if not actions and self.contains_verb(action_text):\n",
    "            return self.extract_actions(action_text)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def extract_clause_actions(self, clause_text, clause_type):\n",
    "        \"\"\"Extract actions from a clause, with special handling for as, while, when clauses\"\"\"\n",
    "        clause_text = clause_text.strip()\n",
    "        \n",
    "        # If clause starts with pronoun, it's likely an action by the subject\n",
    "        if clause_text.lower().startswith(('he ', 'she ', 'they ', 'it ')):\n",
    "            # Remove pronoun\n",
    "            pronoun_match = re.match(r'\\b(he|she|they|it)\\s+', clause_text, re.IGNORECASE)\n",
    "            if pronoun_match:\n",
    "                action_text = clause_text[pronoun_match.end():].strip()\n",
    "                \n",
    "                # Check if it contains a verb\n",
    "                if self.contains_verb(action_text):\n",
    "                    # Try to extract actions\n",
    "                    actions = self.extract_actions(action_text)\n",
    "                    return actions\n",
    "        \n",
    "        # When clause contains key verb patterns like ensures, moves, observes\n",
    "        for keyword, pattern in self.patterns['action_indicators']:\n",
    "            if keyword in clause_text.lower():\n",
    "                match = pattern.search(clause_text)\n",
    "                if match:\n",
    "                    return [match.group(0)]\n",
    "        \n",
    "        # When all else fails, try to find most obvious action\n",
    "        words = clause_text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if is_potential_verb(word):\n",
    "                # Try to extract action fragment, max 5 words\n",
    "                end_idx = min(i + 6, len(words))\n",
    "                action = \" \".join(words[i:end_idx])\n",
    "                return [action]\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def extract_actions(self, text):\n",
    "        \"\"\"Extract discrete actions from text, using POS tagging if available\"\"\"\n",
    "        # Cache computed results\n",
    "        cache_key = f\"extract_{text[:50]}\"\n",
    "        if cache_key in self.action_extraction_cache:\n",
    "            return self.action_extraction_cache[cache_key]\n",
    "            \n",
    "        try:\n",
    "            # Try using NLTK's POS tagger\n",
    "            from nltk import word_tokenize, pos_tag\n",
    "            \n",
    "            tokens = word_tokenize(text)\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            \n",
    "            # Find verb chunks\n",
    "            verb_chunks = []\n",
    "            i = 0\n",
    "            while i < len(pos_tags):\n",
    "                # Look for verbs\n",
    "                if pos_tags[i][1].startswith('VB'):\n",
    "                    # Found a verb, collect verb phrase\n",
    "                    start = i\n",
    "                    verb = pos_tags[i][0]\n",
    "                    i += 1\n",
    "                    \n",
    "                    # Collect auxiliary verbs (if any)\n",
    "                    aux_verbs = []\n",
    "                    while i < len(pos_tags) and (pos_tags[i][1].startswith('VB') or \n",
    "                                                pos_tags[i][0].lower() in ['to', 'not', \"n't\"]):\n",
    "                        aux_verbs.append(pos_tags[i][0])\n",
    "                        i += 1\n",
    "                    \n",
    "                    # Handle passive voice structures\n",
    "                    is_passive = False\n",
    "                    if verb.lower() in ['is', 'are', 'was', 'were', 'be', 'been', 'being'] and i < len(pos_tags):\n",
    "                        # Check if followed by past participle\n",
    "                        if pos_tags[i][1] == 'VBN' or (pos_tags[i][0].lower().endswith('ed') and not pos_tags[i][1].startswith('JJ')):\n",
    "                            is_passive = True\n",
    "                    \n",
    "                    # Collect objects and adverbs that follow\n",
    "                    objects = []\n",
    "                    while i < len(pos_tags) and not pos_tags[i][1].startswith('VB'):\n",
    "                        # Stop at sentence-ending punctuation or conjunctions\n",
    "                        if pos_tags[i][0] in ['.', ',', ';', ':'] or (pos_tags[i][1] == 'CC' and pos_tags[i][0].lower() in ['and', 'or', 'but']):\n",
    "                            break\n",
    "                        objects.append(pos_tags[i][0])\n",
    "                        i += 1\n",
    "                    \n",
    "                    # Build verb phrase\n",
    "                    verb_phrase = verb\n",
    "                    if aux_verbs:\n",
    "                        verb_phrase += \" \" + \" \".join(aux_verbs)\n",
    "                    if objects:\n",
    "                        verb_phrase += \" \" + \" \".join(objects)\n",
    "                    \n",
    "                    # Skip cases that might be nominal compounds\n",
    "                    if self.is_ing_nominal_compound(verb_phrase):\n",
    "                        continue\n",
    "                    \n",
    "                    verb_chunks.append(verb_phrase)\n",
    "                else:\n",
    "                    i += 1\n",
    "            \n",
    "            # Filter for meaningful actions\n",
    "            actions = [self.clean_action_text(chunk) for chunk in verb_chunks if chunk and not self.is_descriptive_statement(chunk)]\n",
    "            actions = [action for action in actions if action]  # Remove any empty strings after cleaning\n",
    "            \n",
    "            if actions:\n",
    "                self.action_extraction_cache[cache_key] = actions\n",
    "                return actions\n",
    "            \n",
    "            # If POS tagging didn't find actions, use standard method\n",
    "            fallback_actions = self.extract_actions_fallback(text)\n",
    "            self.action_extraction_cache[cache_key] = fallback_actions\n",
    "            return fallback_actions\n",
    "            \n",
    "        except Exception:\n",
    "            # If NLTK fails, return regex-based method\n",
    "            fallback_actions = self.extract_actions_fallback(text)\n",
    "            self.action_extraction_cache[cache_key] = fallback_actions\n",
    "            return fallback_actions\n",
    "    \n",
    "    def extract_actions_fallback(self, text):\n",
    "        \"\"\"Fallback method to extract actions using regex patterns\"\"\"\n",
    "        # First clean up text\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Look for specific action patterns\n",
    "        for pattern, extractor in self.patterns['specific_actions']:\n",
    "            match = pattern.search(text)\n",
    "            if match:\n",
    "                return [extractor(match)]\n",
    "        \n",
    "        # Split at common separators\n",
    "        raw_actions = []\n",
    "        separators = r',\\s+|\\s+and\\s+|\\s+then\\s+|\\.\\s+|;\\s+'\n",
    "        parts = re.split(separators, text)\n",
    "        \n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if part:\n",
    "                raw_actions.append(part)\n",
    "        \n",
    "        # Process actions\n",
    "        actions = []\n",
    "        for action in raw_actions:\n",
    "            # Clean up\n",
    "            action = self.clean_action_text(action)\n",
    "            if not action:\n",
    "                continue\n",
    "                \n",
    "            # Skip if it's an -ing noun compound\n",
    "            if self.is_ing_nominal_compound(action):\n",
    "                continue\n",
    "                \n",
    "            # Add if it contains a verb\n",
    "            if self.contains_verb(action) and not self.is_descriptive_statement(action):\n",
    "                actions.append(action)\n",
    "        \n",
    "        # Handle compound actions\n",
    "        if len(actions) > 1:\n",
    "            result = []\n",
    "            prev_verb = None\n",
    "            \n",
    "            for action in actions:\n",
    "                if self.contains_verb(action):\n",
    "                    result.append(action)\n",
    "                    prev_verb = self.extract_main_verb(action)\n",
    "                elif prev_verb:\n",
    "                    # Add verb to this action\n",
    "                    result.append(f\"{prev_verb} {action}\")\n",
    "            \n",
    "            if result:\n",
    "                return result\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def extract_pronoun_actions(self, text):\n",
    "        \"\"\"Extract actions related to pronoun references\"\"\"\n",
    "        # Find pronouns\n",
    "        pronoun_match = re.search(r'\\b(?:he|she|they|it|him|her|them)\\b', text.lower())\n",
    "        \n",
    "        if not pronoun_match:\n",
    "            return []\n",
    "        \n",
    "        # Extract text after pronoun\n",
    "        action_text = text[pronoun_match.end():].strip()\n",
    "        \n",
    "        # Clean up\n",
    "        if action_text.startswith(',') or action_text.startswith('.'):\n",
    "            action_text = action_text[1:].strip()\n",
    "        \n",
    "        if not action_text:\n",
    "            return []\n",
    "        \n",
    "        # Check for \"as\" or \"while\" clauses\n",
    "        clause_splits = {\n",
    "            \" as \": action_text.split(\" as \", 1),\n",
    "            \" while \": action_text.split(\" while \", 1)\n",
    "        }\n",
    "        \n",
    "        for clause_type, parts in clause_splits.items():\n",
    "            if len(parts) > 1:\n",
    "                main_actions = self.extract_actions(parts[0])\n",
    "                clause_actions = self.extract_clause_actions(parts[1], clause_type)\n",
    "                return main_actions + clause_actions\n",
    "        \n",
    "        # Split text into separate actions by conjunctions and commas\n",
    "        action_parts = re.split(r'\\s+and\\s+|\\s*,\\s*|\\s*;\\s*', action_text)\n",
    "        actions = []\n",
    "        \n",
    "        for part in action_parts:\n",
    "            part = part.strip()\n",
    "            if part and self.contains_verb(part):\n",
    "                actions.append(part)\n",
    "        \n",
    "        # Try to capture state and emotion descriptions\n",
    "        state_patterns = [\n",
    "            r'(?:expression|face|mood|emotion)\\s+(?:changes|changed)',\n",
    "            r'appears\\s+to\\s+be\\s+([^.,;:]+)',\n",
    "            r'seems\\s+to\\s+be\\s+([^.,;:]+)',\n",
    "            r'looks\\s+(?:at|to|towards)\\s+([^.,;:]+)\\s+with\\s+interest',\n",
    "            r'engaged\\s+with\\s+([^.,;:]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in state_patterns:\n",
    "            matches = list(re.finditer(pattern, action_text, re.IGNORECASE))\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    state_desc = match.group(0)\n",
    "                    if state_desc not in actions:\n",
    "                        actions.append(state_desc)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def extract_actions_without_subject(self, text):\n",
    "        \"\"\"Extract actions even with no explicit subject mentioned\"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        # Split by conjunctions and punctuation\n",
    "        parts = re.split(r',\\s+|\\s+and\\s+|\\s+then\\s+|;\\s+', text)\n",
    "        \n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "                \n",
    "            # Check if this part contains a verb\n",
    "            if self.contains_verb(part):\n",
    "                # Clean up action text\n",
    "                action = part\n",
    "                action = re.sub(r'^\\s*(?:also|then|finally|next|afterwards|subsequently)\\s+', '', action)\n",
    "                action = action.strip()\n",
    "                \n",
    "                if action:\n",
    "                    actions.append(action)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def extract_actions_from_as_clause(self, text):\n",
    "        \"\"\"Extract actions from text after 'as' that might contain subject+action\"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        # Look for subject at start of \"as\" clause\n",
    "        subject_pattern = r'^\\s*(?:a|an|the)\\s+(?:[a-z\\s]*(?:man|woman|person|boy|girl))\\b'\n",
    "        subj_match = re.search(subject_pattern, text, re.IGNORECASE)\n",
    "        \n",
    "        if subj_match:\n",
    "            # Get text after this subject\n",
    "            after_subj = text[subj_match.end():].strip()\n",
    "            \n",
    "            # Split by conjunctions and commas\n",
    "            parts = re.split(r'\\s+and\\s+|\\s*,\\s*|\\s*;\\s*', after_subj)\n",
    "            \n",
    "            for part in parts:\n",
    "                part = part.strip()\n",
    "                if part and self.contains_verb(part):\n",
    "                    actions.append(part)\n",
    "        else:\n",
    "            # Check if starts with pronoun\n",
    "            pronoun_match = re.search(r'^\\s*(?:he|she|they|it)\\b', text, re.IGNORECASE)\n",
    "            if pronoun_match:\n",
    "                # Get text after pronoun\n",
    "                after_pronoun = text[pronoun_match.end():].strip()\n",
    "                \n",
    "                # Split by conjunctions and commas\n",
    "                parts = re.split(r'\\s+and\\s+|\\s*,\\s*|\\s*;\\s*', after_pronoun)\n",
    "                \n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if part and self.contains_verb(part):\n",
    "                        actions.append(part)\n",
    "            else:\n",
    "                # No explicit subject, just extract verbs\n",
    "                parts = re.split(r'\\s+and\\s+|\\s*,\\s*|\\s*;\\s*', text)\n",
    "                \n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if part and self.contains_verb(part):\n",
    "                        actions.append(part)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def extract_indirect_actions(self, text, subject_ref):\n",
    "        \"\"\"Extract indirectly described actions like state changes, emotional reactions, etc.\"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        # Optimization: if text is too short, return immediately\n",
    "        if len(text) < 10:\n",
    "            return actions\n",
    "            \n",
    "        # Check for matches in text\n",
    "        start_pos = subject_ref[\"end_idx\"] if subject_ref else 0\n",
    "        text_to_check = text[start_pos:] if start_pos < len(text) else text\n",
    "        \n",
    "        # Use precompiled patterns\n",
    "        for pattern, formatter in self.patterns['state_patterns']:\n",
    "            matches = list(pattern.finditer(text_to_check))\n",
    "            for match in matches:\n",
    "                action_text = formatter(match)\n",
    "                if action_text and action_text not in actions:\n",
    "                    actions.append(action_text)\n",
    "                    \n",
    "        # Use precompiled reaction patterns\n",
    "        for pattern, formatter in self.patterns['reaction_patterns']:\n",
    "            matches = list(pattern.finditer(text_to_check))\n",
    "            for match in matches:\n",
    "                action_text = formatter(match)\n",
    "                if action_text and action_text not in actions:\n",
    "                    actions.append(action_text)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def extract_all_actions(self, sentences, subjects, subject_refs, camera_text_to_remove):\n",
    "        \"\"\"Extract all actions for identified subjects, improved version to avoid duplicates and misattribution\"\"\"\n",
    "        all_actions = []\n",
    "        action_idx = 0\n",
    "        current_subject = None\n",
    "        \n",
    "        # Preprocessing: organize subject references by sentence index\n",
    "        subject_refs_by_sentence = defaultdict(list)\n",
    "        for ref in subject_refs:\n",
    "            subject_refs_by_sentence[ref[\"sentence_idx\"]].append(ref)\n",
    "        \n",
    "        # Track already extracted actions for each subject to avoid duplicates\n",
    "        extracted_actions = {subj_id: set() for subj_id in subjects}\n",
    "        \n",
    "        # Process each sentence\n",
    "        for sent_idx, sent in enumerate(sentences):\n",
    "            # Cleanup - create a fully cleaned version\n",
    "            clean_sent = self.remove_camera_parts(sent, camera_text_to_remove)\n",
    "            \n",
    "            # Skip if nothing remains after cleaning\n",
    "            if not clean_sent.strip():\n",
    "                continue\n",
    "                \n",
    "            # Skip purely descriptive sentences\n",
    "            if self.is_scene_description(clean_sent):\n",
    "                continue\n",
    "            \n",
    "            # Get all subject references in this sentence\n",
    "            sentence_refs = subject_refs_by_sentence[sent_idx]\n",
    "            \n",
    "            # Sort by position in sentence\n",
    "            sentence_refs.sort(key=lambda ref: ref[\"start_idx\"])\n",
    "            \n",
    "            if sentence_refs:\n",
    "                # Process each subject in order\n",
    "                for subj_ref in sentence_refs:\n",
    "                    # Find which subject this reference belongs to\n",
    "                    subj_id = self.find_subject_for_reference(subj_ref, subjects)\n",
    "                    if not subj_id:\n",
    "                        continue\n",
    "                        \n",
    "                    current_subject = subj_id\n",
    "                    \n",
    "                    # Extract actions for this subject, passing all subject refs for better boundary detection\n",
    "                    actions = self.extract_subject_actions(clean_sent, subj_ref, sentence_refs)\n",
    "                    \n",
    "                    # Try to capture more indirectly described actions\n",
    "                    indirect_actions = self.extract_indirect_actions(clean_sent, subj_ref)\n",
    "                    for action in indirect_actions:\n",
    "                        if action not in actions:\n",
    "                            actions.append(action)\n",
    "                    \n",
    "                    # Add to subject and chronological list\n",
    "                    for action in actions:\n",
    "                        if action.strip() and not self.is_descriptive_statement(action) and not self.is_ing_nominal_compound(action):\n",
    "                            # Clean up any punctuation\n",
    "                            action = self.clean_action_text(action)\n",
    "                            # Avoid adding duplicate actions for same subject\n",
    "                            if action and action not in extracted_actions[subj_id]:\n",
    "                                extracted_actions[subj_id].add(action)\n",
    "                                subjects[subj_id][\"actions\"].append(action)\n",
    "                                all_actions.append((action_idx, subj_id, action))\n",
    "                                action_idx += 1\n",
    "            \n",
    "                    # Special handling for full sentences that might contain clause actions\n",
    "                    action_idx = self.process_complex_clauses(clean_sent, current_subject, subjects, extracted_actions, all_actions, action_idx)\n",
    "            \n",
    "            # Check for potential \"as\" clauses with implicit subject actions\n",
    "            elif current_subject and ' as ' in clean_sent:\n",
    "                as_parts = clean_sent.split(' as ', 1)\n",
    "                if len(as_parts) > 1 and self.contains_clear_action(as_parts[1]):\n",
    "                    # Extract actions from part after \"as\"\n",
    "                    actions = self.extract_actions_from_as_clause(as_parts[1])\n",
    "                    \n",
    "                    for action in actions:\n",
    "                        if action.strip() and not self.is_descriptive_statement(action) and not self.is_ing_nominal_compound(action):\n",
    "                            action = self.clean_action_text(action)\n",
    "                            if action and action not in extracted_actions[current_subject]:\n",
    "                                extracted_actions[current_subject].add(action)\n",
    "                                subjects[current_subject][\"actions\"].append(action)\n",
    "                                all_actions.append((action_idx, current_subject, action))\n",
    "                                action_idx += 1\n",
    "            \n",
    "            # If no explicit subject but we have current subject, check for pronoun references\n",
    "            elif current_subject and self.contains_pronoun(clean_sent):\n",
    "                # Extract actions for pronoun\n",
    "                actions = self.extract_pronoun_actions(clean_sent)\n",
    "                \n",
    "                for action in actions:\n",
    "                    if action.strip() and not self.is_descriptive_statement(action) and not self.is_ing_nominal_compound(action):\n",
    "                        # Clean up any punctuation\n",
    "                        action = self.clean_action_text(action)\n",
    "                        if action and action not in extracted_actions[current_subject]:\n",
    "                            extracted_actions[current_subject].add(action)\n",
    "                            subjects[current_subject][\"actions\"].append(action)\n",
    "                            all_actions.append((action_idx, current_subject, action))\n",
    "                            action_idx += 1\n",
    "            \n",
    "            # When no explicit mention but action exists, check for implicit subject\n",
    "            elif self.contains_clear_action(clean_sent) and current_subject:\n",
    "                # Extract actions without explicit subject mentions\n",
    "                actions = self.extract_actions_without_subject(clean_sent)\n",
    "                \n",
    "                for action in actions:\n",
    "                    if action.strip() and not self.is_descriptive_statement(action) and not self.is_ing_nominal_compound(action):\n",
    "                        # Clean up any punctuation\n",
    "                        action = self.clean_action_text(action)\n",
    "                        if action and action not in extracted_actions[current_subject]:\n",
    "                            extracted_actions[current_subject].add(action)\n",
    "                            subjects[current_subject][\"actions\"].append(action)\n",
    "                            all_actions.append((action_idx, current_subject, action))\n",
    "                            action_idx += 1\n",
    "        \n",
    "        return all_actions\n",
    "    \n",
    "    def process_complex_clauses(self, sentence, current_subject, subjects, extracted_actions, all_actions, action_idx):\n",
    "        \"\"\"Process clause actions in complex sentences\"\"\"\n",
    "        if not current_subject:\n",
    "            return action_idx\n",
    "        \n",
    "        # 1. Process clause markers (as, while, when, etc.)\n",
    "        clause_markers = [' as ', ' while ', ' when ']\n",
    "        \n",
    "        for marker in clause_markers:\n",
    "            if marker in sentence:\n",
    "                parts = sentence.split(marker, 1)\n",
    "                if len(parts) == 2:\n",
    "                    clause = parts[1].strip()\n",
    "                    \n",
    "                    # Case 1: Pronoun-led actions (he, she, they)\n",
    "                    if clause.lower().startswith(('he ', 'she ', 'they ')):\n",
    "                        clause_actions = self.extract_clause_actions(clause, marker.strip())\n",
    "                        for action in clause_actions:\n",
    "                            action_idx = self._process_and_add_action(\n",
    "                                action, current_subject, subjects, extracted_actions, all_actions, action_idx\n",
    "                            )\n",
    "                    \n",
    "                    # Case 2: Adverb+ing form actions (occasionally watching)\n",
    "                    adverb_ing_match = re.search(r'\\b(\\w+ly\\s+\\w+ing[^.,;:]*)', clause)\n",
    "                    if adverb_ing_match:\n",
    "                        action = adverb_ing_match.group(1)\n",
    "                        action_idx = self._process_and_add_action(\n",
    "                            action, current_subject, subjects, extracted_actions, all_actions, action_idx\n",
    "                        )\n",
    "        \n",
    "        # 2. Check for ensuring behaviors (ensuring, adjusting, making sure)\n",
    "        assurance_patterns = [\n",
    "            r'\\bensuring\\s+[^.,;:]+',\n",
    "            r'\\badjusting\\s+[^.,;:]+',\n",
    "            r'\\bmaking\\s+sure\\s+[^.,;:]+',\n",
    "            r'\\bensures\\s+that\\s+[^.,;:]+',\n",
    "            r'\\bensures\\s+the\\s+[^.,;:]+' \n",
    "        ]\n",
    "        \n",
    "        for pattern in assurance_patterns:\n",
    "            matches = re.finditer(pattern, sentence, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                action = match.group(0)\n",
    "                action_idx = self._process_and_add_action(\n",
    "                    action, current_subject, subjects, extracted_actions, all_actions, action_idx\n",
    "                )\n",
    "        \n",
    "        return action_idx\n",
    "    \n",
    "    def _process_and_add_action(self, action, subject, subjects, extracted_actions, all_actions, action_idx):\n",
    "        \"\"\"Process and add action to subject, including all necessary checks\"\"\"\n",
    "        if action and action.strip() and not self.is_descriptive_statement(action) and not self.is_ing_nominal_compound(action):\n",
    "            # Clean up any punctuation\n",
    "            action = self.clean_action_text(action)\n",
    "            if action and action not in extracted_actions[subject]:\n",
    "                extracted_actions[subject].add(action)\n",
    "                subjects[subject][\"actions\"].append(action)\n",
    "                all_actions.append((action_idx, subject, action))\n",
    "                action_idx += 1\n",
    "        return action_idx\n",
    "    \n",
    "    def parse_video_caption(self, caption_data):\n",
    "        \"\"\"Parse video captions to extract structured information\"\"\"\n",
    "        # Reset all caches\n",
    "        self.reset_cache()\n",
    "        \n",
    "        for key in caption_data:\n",
    "            caption = caption_data[key]\n",
    "\n",
    "        # caption = caption_data[\"caption\"]\n",
    "        \n",
    "        # Initialize result structure\n",
    "        result = {\n",
    "            f\"{key}\": {\n",
    "                \"model_caption\": f\"{caption}\",\n",
    "                \"camera_motion\": \"\",\n",
    "                \"num_subjects\": \"\",\n",
    "                \"motion_list\": \"\",\n",
    "                \"chronological_motion_list\": \"\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # result[f\"{key}\"][\"model_caption\"] = \", \".join(caption)\n",
    "\n",
    "        # Split caption into sentences\n",
    "        sentences = sent_tokenize(caption)\n",
    "        \n",
    "        # Extract camera motions, maintaining order\n",
    "        camera_motions = []\n",
    "        camera_text_to_remove = []\n",
    "        \n",
    "        for sent in sentences:\n",
    "            motions, text_to_remove = self.extract_camera_motions_from_sentence(sent)\n",
    "            if motions:  # Only add non-empty results\n",
    "                camera_motions.extend(motions)\n",
    "                camera_text_to_remove.extend(text_to_remove)\n",
    "        \n",
    "        if camera_motions:\n",
    "            result[f\"{key}\"][\"camera_motion\"] = \", \".join(camera_motions)\n",
    "        \n",
    "        # Cache camera text for reuse\n",
    "        self.cached_camera_motions = camera_motions\n",
    "        self.cached_camera_texts = camera_text_to_remove\n",
    "        \n",
    "        # First pass: identify all subjects across text (both original and camera-cleaned text)\n",
    "        subjects, subject_refs = self.identify_all_subjects(sentences, camera_text_to_remove)\n",
    "        \n",
    "        # Second pass: process actions for identified subjects\n",
    "        all_actions = self.extract_all_actions(sentences, subjects, subject_refs, camera_text_to_remove)\n",
    "        \n",
    "        # If no subjects identified, try a more relaxed approach\n",
    "        if not subjects:\n",
    "            # Use more generic subject identification\n",
    "            subjects, subject_refs = self.identify_subjects_generic(sentences, camera_text_to_remove)\n",
    "            all_actions = self.extract_all_actions(sentences, subjects, subject_refs, camera_text_to_remove)\n",
    "        \n",
    "        # Format output\n",
    "        result[f\"{key}\"][\"num_subjects\"] = \"Single subject\" if len(subjects) == 1 else f\"Multiple subjects ({len(subjects)})\"\n",
    "        \n",
    "        # Format motion lists by subject\n",
    "        motion_lists = []\n",
    "        for subj_id, subj_info in subjects.items():\n",
    "            base_type = subj_info[\"base_noun\"].capitalize()\n",
    "            actions = ', '.join(subj_info[\"actions\"])\n",
    "            motion_lists.append(f\"{subj_id}: {base_type} [{actions}]\")\n",
    "        \n",
    "        result[f\"{key}\"][\"motion_list\"] = \"\\n\".join(motion_lists)\n",
    "        \n",
    "        # Add chronological action list\n",
    "        all_actions.sort(key=lambda x: x[0])  # Sort by index\n",
    "        chrono_actions = [f\"{action} ({subj_id})\" for _, subj_id, action in all_actions]\n",
    "        \n",
    "        result[f\"{key}\"][\"chronological_motion_list\"] = \", \".join(chrono_actions)\n",
    "        \n",
    "        return result\n",
    "\n",
    "def parse_video_caption(caption_data):\n",
    "    \"\"\"Public interface function that creates a parser instance and performs parsing\"\"\"\n",
    "    parser = VideoDescriptionParser()\n",
    "    return parser.parse_video_caption(caption_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case 1:\n",
      "{'7R8ZU': 'The recorder moves forward, the girl closes the laptop with her right hand, stands up and walks backward, then turns around and walks forward, the recorder moves backward, the girl walks to the left, and stirs the food with a spatula in her right hand.'}\n",
      "Processing time: 0.0330 seconds\n",
      "{\n",
      "    \"7R8ZU\": {\n",
      "        \"model_caption\": \"The recorder moves forward, the girl closes the laptop with her right hand, stands up and walks backward, then turns around and walks forward, the recorder moves backward, the girl walks to the left, and stirs the food with a spatula in her right hand.\",\n",
      "        \"camera_motion\": \"\",\n",
      "        \"num_subjects\": \"Multiple subjects (2)\",\n",
      "        \"motion_list\": \"Subject 1: Recorder [moves forward, moves backward]\\nSubject 2: Girl [closes the laptop with her right hand, stands up, walks backward, turns around, walks forward, walks to the left, stirs the food with a spatula in her right hand]\",\n",
      "        \"chronological_motion_list\": \"moves forward (Subject 1), closes the laptop with her right hand (Subject 2), stands up (Subject 2), walks backward (Subject 2), turns around (Subject 2), walks forward (Subject 2), moves backward (Subject 1), walks to the left (Subject 2), stirs the food with a spatula in her right hand (Subject 2)\"\n",
      "    }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test case 2:\n",
      "{'539EH': 'The camera keeps stationary. The man picks up a box with both hands, then opens it with his left hand. He takes out a shoe from the box with his right hand and puts it on his foot. He then picks up another shoe with his right hand and puts it on his other foot. The man then picks up the box with both hands and turns around to sit on a chair. He picks up a phone with his right hand and makes a phone call. '}\n",
      "Processing time: 0.0088 seconds\n",
      "{\n",
      "    \"539EH\": {\n",
      "        \"model_caption\": \"The camera keeps stationary. The man picks up a box with both hands, then opens it with his left hand. He takes out a shoe from the box with his right hand and puts it on his foot. He then picks up another shoe with his right hand and puts it on his other foot. The man then picks up the box with both hands and turns around to sit on a chair. He picks up a phone with his right hand and makes a phone call. \",\n",
      "        \"camera_motion\": \"Camera keeps stationary\",\n",
      "        \"num_subjects\": \"Single subject\",\n",
      "        \"motion_list\": \"Subject 1: Man [picks up a box with both hands, opens it with his left hand, takes out a shoe from the box with his right hand, puts it on his foot, picks up another shoe with his right hand, puts it on his other foot, picks up the box with both hands, turns around to sit on a chair, picks up a phone with his right hand, makes a phone call]\",\n",
      "        \"chronological_motion_list\": \"picks up a box with both hands (Subject 1), opens it with his left hand (Subject 1), takes out a shoe from the box with his right hand (Subject 1), puts it on his foot (Subject 1), picks up another shoe with his right hand (Subject 1), puts it on his other foot (Subject 1), picks up the box with both hands (Subject 1), turns around to sit on a chair (Subject 1), picks up a phone with his right hand (Subject 1), makes a phone call (Subject 1)\"\n",
      "    }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test case 3:\n",
      "{'4_08_000048': 'The girl speaks while playing with a frisbee in her hand, then tosses it aside, turns her head to look left, and the man blinks. '}\n",
      "Processing time: 0.0027 seconds\n",
      "{\n",
      "    \"4_08_000048\": {\n",
      "        \"model_caption\": \"The girl speaks while playing with a frisbee in her hand, then tosses it aside, turns her head to look left, and the man blinks. \",\n",
      "        \"camera_motion\": \"\",\n",
      "        \"num_subjects\": \"Multiple subjects (2)\",\n",
      "        \"motion_list\": \"Subject 1: Girl [speaks, tosses it aside, turns her head to look left]\\nSubject 2: Man [blinks]\",\n",
      "        \"chronological_motion_list\": \"speaks (Subject 1), tosses it aside (Subject 1), turns her head to look left (Subject 1), blinks (Subject 2)\"\n",
      "    }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test case 4:\n",
      "{'0AYPZ': 'The woman walks around, picks up the vacuum cleaner, turns it on, and starts cleaning. The light outside changes.'}\n",
      "Processing time: 0.0022 seconds\n",
      "{\n",
      "    \"0AYPZ\": {\n",
      "        \"model_caption\": \"The woman walks around, picks up the vacuum cleaner, turns it on, and starts cleaning. The light outside changes.\",\n",
      "        \"camera_motion\": \"\",\n",
      "        \"num_subjects\": \"Single subject\",\n",
      "        \"motion_list\": \"Subject 1: Woman [walks around, picks up the vacuum cleaner, turns it on, starts cleaning]\",\n",
      "        \"chronological_motion_list\": \"walks around (Subject 1), picks up the vacuum cleaner (Subject 1), turns it on (Subject 1), starts cleaning (Subject 1)\"\n",
      "    }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test case 5:\n",
      "{'7O6FK': 'The recorder moves forward, the man in blue shirt picks up a bag with his right hand and throws it down, then he turns around to pick up a picture frame with both hands and walks down the stairs.'}\n",
      "Processing time: 0.0056 seconds\n",
      "{\n",
      "    \"7O6FK\": {\n",
      "        \"model_caption\": \"The recorder moves forward, the man in blue shirt picks up a bag with his right hand and throws it down, then he turns around to pick up a picture frame with both hands and walks down the stairs.\",\n",
      "        \"camera_motion\": \"\",\n",
      "        \"num_subjects\": \"Multiple subjects (2)\",\n",
      "        \"motion_list\": \"Subject 1: Recorder [moves forward]\\nSubject 2: Man [picks up a bag with his right hand, throws it down, he turns around to pick up a picture frame with both hands, walks down the stairs]\",\n",
      "        \"chronological_motion_list\": \"moves forward (Subject 1), picks up a bag with his right hand (Subject 2), throws it down (Subject 2), he turns around to pick up a picture frame with both hands (Subject 2), walks down the stairs (Subject 2)\"\n",
      "    }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Test case 6:\n",
      "{'26k-11-2-3|P1|3561|3966': \"The recorder's right hand turns off the faucet, then picks up a cup and pours water into it. The left hand takes the cup while the right hand opens the cabinet door, placing the cup inside with the left hand. The right hand then closes the cabinet door, turns around, walks to the table, and picks up a bottle with the right hand. The left hand takes the bottle while the right hand opens the cap, then turns around again. The right hand places the cap on the table, then turns on the faucet. Both hands wash the bottle, and subsequently, the right hand turns off the faucet.\"}\n",
      "Processing time: 0.0079 seconds\n",
      "{\n",
      "    \"26k-11-2-3|P1|3561|3966\": {\n",
      "        \"model_caption\": \"The recorder's right hand turns off the faucet, then picks up a cup and pours water into it. The left hand takes the cup while the right hand opens the cabinet door, placing the cup inside with the left hand. The right hand then closes the cabinet door, turns around, walks to the table, and picks up a bottle with the right hand. The left hand takes the bottle while the right hand opens the cap, then turns around again. The right hand places the cap on the table, then turns on the faucet. Both hands wash the bottle, and subsequently, the right hand turns off the faucet.\",\n",
      "        \"camera_motion\": \"\",\n",
      "        \"num_subjects\": \"Single subject\",\n",
      "        \"motion_list\": \"Subject 1: Recorder ['s right hand turns off the faucet, picks up a cup, pours water into it, The left hand takes the cup, placing the cup inside with the left hand, The right hand, closes the cabinet door, turns around, walks to the table, picks up a bottle with the right hand, The left hand takes the bottle, turns around again, The right hand places the cap on the table, turns on the faucet, Both hands wash the bottle, the right hand turns off the faucet]\",\n",
      "        \"chronological_motion_list\": \"'s right hand turns off the faucet (Subject 1), picks up a cup (Subject 1), pours water into it (Subject 1), The left hand takes the cup (Subject 1), placing the cup inside with the left hand (Subject 1), The right hand (Subject 1), closes the cabinet door (Subject 1), turns around (Subject 1), walks to the table (Subject 1), picks up a bottle with the right hand (Subject 1), The left hand takes the bottle (Subject 1), turns around again (Subject 1), The right hand places the cap on the table (Subject 1), turns on the faucet (Subject 1), Both hands wash the bottle (Subject 1), the right hand turns off the faucet (Subject 1)\"\n",
      "    }\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Results saved to extract_results/LLM-free_case.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def test_parser(test_case, output_path):\n",
    "    test_case = test_case\n",
    "    output_file_path = output_path\n",
    "    all_results = {}\n",
    "    timeout_seconds = 10\n",
    "\n",
    "    if os.path.exists(output_file_path):\n",
    "        with open(output_file_path, 'r') as f:\n",
    "            try:\n",
    "                all_results = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not decode existing JSON file at {output_file_path}. Starting with an empty result set.\")\n",
    "                all_results = {}\n",
    "        \n",
    "    for i, test_case in enumerate(test_case):\n",
    "        print(f\"Test case {i+1}:\")\n",
    "        print(test_case)\n",
    "        start_time = time.time()\n",
    "        result = parse_video_caption(test_case)\n",
    "        end_time = time.time()\n",
    "        print(f\"Processing time: {end_time - start_time:.4f} seconds\")\n",
    "        print(json.dumps(result, indent=4))\n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "        all_results.update(result) # Use update to merge the new result into existing results\n",
    "    \n",
    "    with open(output_file_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=4)\n",
    "    print(f\"Results saved to {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_case = [\n",
    "{\"7R8ZU\": \"The recorder moves forward, the girl closes the laptop with her right hand, stands up and walks backward, then turns around and walks forward, the recorder moves backward, the girl walks to the left, and stirs the food with a spatula in her right hand.\"},\n",
    "{\"539EH\": \"The camera keeps stationary. The man picks up a box with both hands, then opens it with his left hand. He takes out a shoe from the box with his right hand and puts it on his foot. He then picks up another shoe with his right hand and puts it on his other foot. The man then picks up the box with both hands and turns around to sit on a chair. He picks up a phone with his right hand and makes a phone call. \"},\n",
    "{\"4_08_000048\": \"The girl speaks while playing with a frisbee in her hand, then tosses it aside, turns her head to look left, and the man blinks. \"},\n",
    "{\"0AYPZ\": \"The woman walks around, picks up the vacuum cleaner, turns it on, and starts cleaning. The light outside changes.\"},\n",
    "\n",
    "{\"7O6FK\": \"The recorder moves forward, the man in blue shirt picks up a bag with his right hand and throws it down, then he turns around to pick up a picture frame with both hands and walks down the stairs.\"},\n",
    "{\"26k-11-2-3|P1|3561|3966\": \"The recorder's right hand turns off the faucet, then picks up a cup and pours water into it. The left hand takes the cup while the right hand opens the cabinet door, placing the cup inside with the left hand. The right hand then closes the cabinet door, turns around, walks to the table, and picks up a bottle with the right hand. The left hand takes the bottle while the right hand opens the cap, then turns around again. The right hand places the cap on the table, then turns on the faucet. Both hands wash the bottle, and subsequently, the right hand turns off the faucet.\"},\n",
    "    ]\n",
    "    os.makedirs('extract_results', exist_ok=True)\n",
    "    output_path = \"extract_results/LLM-free_case.json\"\n",
    "    test_parser(test_case, output_path=output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lf",
   "language": "python",
   "name": "lf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
